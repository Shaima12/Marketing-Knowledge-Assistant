[
  {
    "url": "https://www.searchenginejournal.com/google-updates-javascript-seo-docs-with-canonical-advice/563545/",
    "title": "Google Updates JavaScript SEO Docs With Canonical Advice via @sejournal, @MattGSouthern",
    "date": "2025-12-17T14:56:03+00:00",
    "content": "Google updated its JavaScript SEO documentation with new guidance on handling canonical URLs for JavaScript-rendered sites.\nThe documentation update also adds corresponding guidance to Google’s best practices for consolidating duplicate URLs .\nThe updated documentation focuses on a timing issue specific to JavaScript sites: canonicalization can happen twice during Google’s processing.\nGoogle evaluates canonical signals once when it first crawls the raw HTML, then again after rendering the JavaScript. If your raw HTML contains one canonical URL and your JavaScript sets a different one, Google may receive conflicting signals.\nThe documentation notes that injecting canonical tags via JavaScript is supported but not recommended. When JavaScript sets a canonical URL, Google can pick it up during rendering, but incorrect implementations can cause issues.\nMultiple canonical tags, or changes to an existing canonical tag during rendering, can lead to unexpected indexing results.\nGoogle recommends two best practices depending on your site’s architecture.\nThe preferred method is setting the canonical URL in the raw HTML response to match the URL your JavaScript will ultimately render. This gives Google consistent signals before and after rendering.\nIf JavaScript must set a different canonical URL, Google recommends leaving the canonical tag out of the initial HTML. This can help avoid conflicting signals between the crawl and render phases.\nThe documentation also reminds developers to ensure only one canonical tag exists on any given page after rendering.\nThis guidance addresses a subtle detail that can be easy to miss when managing JavaScript-rendered sites.\nThe gap between when Google crawls your raw HTML and when it renders your JavaScript creates an opportunity for canonical signals to diverge.\nIf you use frameworks like React, Vue, or Angular that handle routing and page structure client-side, it’s worth checking how your canonical tags are implemented. Look at whether your server response includes a canonical tag and whether your JavaScript modifies or duplicates it.\nIn many cases, the fix is to coordinate your server-side and client-side canonical implementations so they send the same signal at both stages of Google’s processing.\nThis documentation update clarifies behavior that may not have been obvious before. It doesn’t change how Google processes canonical tags.\nIf you’re seeing unexpected canonical selection in Search Console’s Page indexing reporting, check for mismatches between your raw HTML and rendered canonical tags. The URL Inspection tool shows both the raw and rendered HTML, which makes it possible to compare canonical implementations across both phases."
  },
  {
    "url": "https://www.searchenginejournal.com/the-facts-about-trust-change-everything-about-link-building/563530/",
    "title": "The Facts About Trust Change Everything About Link Building via @sejournal, @martinibuster",
    "date": "2025-12-17T14:21:24+00:00",
    "content": "Trust is commonly understood to be a standalone quality that is passed between sites regardless of link neighborhood or topical vertical. What I’m going to demonstrate is that “trust” is not a thing that trickles down from a trusted site to another site. The implication for link building is that many may have been focusing on the wrong thing.\nSix years ago I was the first person to write about link distance ranking algorithms that are a way to create a map of the Internet that begins with a group of sites that are judged to be trustworthy. These sites are called the seed set. The seed set links to other sites, which in turn link to ever increasing groups of other sites. The sites closer to the original seed set tend to be trustworthy websites. The sites that are furthest away from the seed set tend to be not trustworthy.\nGoogle still counts links as part of the ranking process so it’s likely that there continues to be a seed set that is considered trustworthy from which the further away you a site is linked from the seeds the likelier it is considered to be spam.\nCircling back to the idea of trust as a ranking related factor, trust is not a thing that is passed from one site to another. Trust, in this context, is not even a part of the conversation. Sites are said to be trustworthy by the link distance between the site in question and the original seed set. So you see, there is no trust that is conveyed from one site or another.\nThe word Trustworthiness is even a part of the E-E-A-T standard of what constitutes a quality website. So trust should never be considered as a thing that is passed from one site to another because it does not exist.\nThe takeaway is that link building decisions based on the idea of trust propagated through links are built on an outdated premise. What matters is whether a site sits close to trusted seed sites within the same topical neighborhood, not whether it receives a link from a widely recognized or authoritative domain. This insight transforms link evaluation into a relevance problem rather than a reputation problem. This insight should encourage site owners to focus on earning links that reinforce topical alignment instead of chasing links that appear impressive but have little, if any, ranking value.\nThe second thing about the link distance ranking algorithms that I think is quite cool and elegant is that websites naturally coalesce around each other according to their topics. Some topics are highly linked and some, like various business association verticals, are not well linked at all. The consequence is that those poorly linked sites that are nevertheless close to the original seed set do not acquire much “link equity” because their link neighborhoods are so small.\nWhat that means is that a low-linked vertical can be a part of the original seed set and display low third-party authority metrics scores. The implication is that the third-party link metrics that measure how many inbound links a site has fail. They fail because third-party authority metrics follow the old and outdated PageRank scoring method that counts the amount of inbound links a site has. PageRank was created around 1998 and is so old that the patent on it has expired.\nThe seed set paradigm does not measure inbound links. It measures the distance from sites that are judged to be trustworthy. That has nothing to do with how many links those seed set sites have and everything to do with them being trustworthy, which is a subjective judgment.\nThat’s why I say that third-party link authority metrics are outdated. They don’t follow the seed set paradigm, they follow the old and outdated PageRank paradigm. The insight to take away from this is that many highly trustworthy sites are being overlooked for link building purposes because link builders are judging the quality of a site by outdated metrics that incorrectly devalue sites in verticals that aren’t well linked but are actually very close to the trustworthy seed set.\nLet’s circle back to the observation that websites tend to naturally link to other sites that are on the same topic. What’s interesting about this is that the seed sets can be chosen according to topic verticals. Some verticals have a lot of inbound links and some verticals are in their own little corner of the Internet and aren’t link to from outside of their clique.\nA link distance ranking algorithm can thus be used to calculate the relevance according to whatever neighbhorhood a site is located in. Majestic does something like that with their Trust Flow and Topical Trust Flow metrics that actually start with trusted seed sites. Topical Trust Flow breaks that score down into specific topic categories. The Topical Trust Flow metric shows how relevant a website is for a given metric.\nMy point isn’t that you should use that metric, although I think it’s the best one available today. The point is that there is no context for thinking about trustworthiness as something that spreads from link to link.\nOnce you can think of links in the paradigm of distance within a topic category it becomes easier to understand why a link from a university website or some other so-called “high trust” site isn’t necessarily that good or useful. I know for certain because there was a time before distance ranking where the topic of the site didn’t matter but now it does matter very much and it has mattered for a long time now.\nFor example, a site like The Washington Post is not a part of the Credit Repair niche. Any “trust” that may be calculated from a New York Times link to a Credit Repair site will likely be dampened to zero. Of course it will. Remember, seed set trust distance is calculated within groups within a niche. There is no trust passed from one link to another link. It is only the distance that is counted.\nLogically, it makes sense to assume that there will be no validating effect between irrelevant sites. relevant website for the purposes of the seed set trust calculations.\nModern link evaluation is about topical proximity, not “trust” or raw link counts. Search systems measure how close a site is to trusted seed sites within its own topic neighborhood, which means relevant links from smaller, niche sites can matter more than links from famous but unrelated domains.\nThis knowledge should enable smarter link building by focusing efforts on contextually relevant websites that may actually strengthen relevance and rankings, instead of chasing outdated link authority scores that no longer reflect how search works."
  },
  {
    "url": "https://martech.org/62-of-b2b-cmos-not-ready-to-compete-against-ai-enabled-companies-report/",
    "title": "62% of B2B CMOs not ready to compete against AI-enabled companies: report",
    "date": "2025-12-17T15:01:51+00:00",
    "content": "Two-thirds of CMOs say they lack the skills, budget or resources to compete against faster-moving, AI-enabled challengers, according to new research from the agency 3Thinkrs.\nThat’s because the acceleration of AI-generated search and decision-making is rapidly eroding traditional marketing levers. With brand visibility collapsing across search, websites and social, the race is on to modernize strategies for the new era of AI-first discovery.\nThe findings come from “ A CMO’s Marketing and Communications Playbook for 2026 ,” a survey of 400 B2B tech marketing leaders — and they paint a picture of a playbook that’s falling apart faster than many teams expected.\nAccording to the report, B2B tech websites experienced a 34% decline in traffic between 2024 and 2025, despite AI-generated traffic accelerating toward an expected 20% share by the end of 2025. That is a significant erosion of the traditional content ladder that marketers have depended on for decades.\nDig deeper: 5 capabilities that separate AI-native teams from everyone else\nSearch is changing, too. The study projects that by 2027, traditional search will account for just 45% of all search queries, a 42% drop from earlier norms. Social media channels are also feeling the impact. On LinkedIn, the visibility of organic company content has slipped sharply — from 2.1% to 1.6% of users’ feeds between March and October 2025 — shrinking the reach of content that once drove awareness and engagement.\nWith generative engines replacing traditional search behavior, CMOs are being forced to pivot. Generative Engine Optimization (GEO) is now the must-have capability for teams trying to stay visible in zero-click environments.\nSixty-one percent of marketing leaders are already adapting their strategies to improve visibility in AI-generated summaries and answer engines. The shift is moving marketing from keyword placement to “confidence signaling” — ensuring that AI models recognize your brand as credible, current and authoritative.\nAs generative search tools synthesize responses from across the web, brands that lack a clear, unified narrative risk being misrepresented — or ignored altogether.\nAnd that’s a real problem: 61% of CMOs say their company is only “mildly proficient” or worse at maintaining a shared brand story across PR, content and sales. Without a strong narrative, even great content may fail to earn visibility.\nDig deeper: How to speed up AI adoption and turn hype into results\nMarketers are responding by leaning into structured storytelling: think contrast-based messaging (problem/solution), the rule of three, or clear hero/villain arcs. Repetition and rhythm — across every channel — are becoming key to driving recall and reinforcing trust.\nCMOs are also doubling down on more engaging formats. With LinkedIn organic reach declining, 54% say they’re prioritizing short-form video and multimedia content as a way to reclaim relevance.\nWhile most of the focus has been on visibility, AI has also introduced a whole new category of reputational risk: synthetic media.\nGenerative AI makes it easy to produce realistic — but fake — video and audio content. Despite the threat, only 16% of B2B tech brands have a general crisis communications playbook, and just 14% have a specific response plan for AI-generated threats like deepfakes.\nWhat’s worse, 26% of CMOs say they’d rely solely on their internal teams to handle such a crisis — even while acknowledging the lack of formal procedures. That leaves most brands dangerously exposed in a landscape where misinformation spreads fast and credibility is everything."
  }
]