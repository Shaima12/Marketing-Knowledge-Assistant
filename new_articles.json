[
  {
    "url": "https://www.searchenginejournal.com/googles-new-user-intent-extraction-method/565840/",
    "title": "Google’s New User Intent Extraction Method via @sejournal, @martinibuster",
    "date": "2026-01-26T10:51:40+00:00",
    "content": "Google published a research paper on how to extract user intent from user interactions that can then be used for autonomous agents. The method they discovered uses on-device small models that do not need to send data back to Google, which means that a user’s privacy is protected.\nThe researchers discovered they were able to solve the problem by splitting it into two tasks. Their solution worked so well it was able to beat the base performance of multi-modal large language models (MLLMs) in massive data centers.\nThe focus of the research is on identifying the user intent through the series of actions that a user takes on their mobile device or browser while also keeping that information on the device so that no information is sent back to Google. That means the processing must happen on the device.\n“…our two-stage approach demonstrates superior performance compared to both smaller models and a state-of-the-art large MLLM, independent of dataset and model type. Our approach also naturally handles scenarios with noisy data that traditional supervised fine-tuning methods struggle with.”\nIntent extraction from screenshots and text descriptions of user interactions was a technique that was proposed in 2025 using Multimodal Large Language Models (MLLMs). The researchers say they followed this approach to their problem but using an improved prompt.\nThe researchers explained that extracting intent is not a trivial problem to solve and that there are multiple errors that can happen along the steps. The researchers use the word trajectory to describe a user journey within a mobile or web application, represented as a sequence of interactions.\nThe user journey (trajectory) is turned into a formula where each interaction step consists of two parts:\nThey described three qualities of a good extracted intent:\nThe researchers explain that grading extracted intent is difficult because user intents contain complex details (like dates or transaction data) and the user intents are inherently subjective, containing ambiguities, which is a hard problem to solve. The reason trajectories are subjective is because the underlying motivations are ambiguous.\nFor example, did a user choose a product because of the price or the features? The actions are visible but the motivations are not. Previous research shows that intents between humans matched 80% on web trajectories and 76% on mobile trajectories, so it’s not like a given trajectory can always indicate a specific intent.\nAfter ruling out other methods like Chain of Thought (CoT) reasoning (because small language models struggled with the reasoning), they chose a two-stage approach that emulated Chain of Thought reasoning.\nThe researchers explained their two-stage approach:\n“First, we use prompting to generate a summary for each interaction (consisting of a visual screenshot and textual action representation) in a trajectory. This stage is prompt-based as there is currently no training data available with summary labels for individual interactions.\nSecond, we feed all of the interaction-level summaries into a second stage model to generate an overall intent description. We apply fine-tuning in the second stage…”\nThe first summary, for the screenshot of the interaction, they divide the summary into two parts, but there is also a third part.\nThe third component (speculative intent) is a way to get rid of speculation about the user’s intent, where the model is basically guessing at what’s going on. This third part is labeled “speculative intent” and they actually just get rid of it. Surprisingly, allowing the model to speculate and then getting rid of that speculation leads to a higher quality result.\nThe researchers cycled through multiple prompting strategies and this was the one that worked the best.\nFor the second stage, the researchers fine tuned a model for generating an overall intent description. They fine tuned the model with training data that is made up of two parts:\nThe model initially tended to hallucinate because the first part (input summaries) are potentially incomplete, while the “target intents” are complete. That caused the model to learn to fill in the missing parts in order to make the input summaries match the target intents.\nThey solved this problem by “refining” the target intents by removing details that aren’t reflected in the input summaries. This trained the model to infer the intents based only on the inputs.\nThe researchers compared four different approaches and settled on this approach because it performed so well.\nThe research paper ends by summarizing potential ethical issues where an autonomous agent might take actions that are not in the user’s interest and stressed the necessity to build the proper guardrails.\nThe authors also acknowledged limitations in the research that might limit generalizability of the results. For example, the testing was done only on Android and web environments, which means that the results might not generalize to Apple devices. Another limitation is that the research was limited to users in the United States in the English language.\nThere is nothing in the research paper or the accompanying blog post that suggests that these processes for extracting user intent are currently in use. The blog post ends by communicating that the described approach is helpful:\n“Ultimately, as models improve in performance and mobile devices acquire more processing power, we hope that on-device intent understanding can become a building block for many assistive features on mobile devices going forward.”\nNeither the blog post about this research or the research paper itself describe the results of these processes as something that might be used in AI search or classic search. It does mention the context of autonomous agents.\nThe research paper explicitly mentions the context of an autonomous agent on the device that is observing how the user is interacting with a user interface and then be able to infer what the goal (the intent) of those actions are.\nThe paper lists two specific applications for this technology:\nWhile this might not be used right away, it shows the direction that Google is heading, where small models on a device will be watching user interactions and sometimes stepping in to assist users based on their intent. Intent here is used in the sense of understanding what a user is trying to do.\nSmall models, big results: Achieving superior intent extraction through decomposition\nSmall Models, Big Results: Achieving Superior Intent Extraction through Decomposition ( PDF )"
  },
  {
    "url": "https://embryo.com/blog/why-is-ai-so-confident-when-its-wrong/",
    "title": "Why is AI so confident when it’s wrong?",
    "date": "2026-01-26T10:41:42+00:00",
    "content": "Let me take you back to spring 2000: NSYNC has made it into the Top 5 in the singles charts with ‘Bye Bye Bye’, Richard Branson receives his knighthood, and, officially, the dot-com bubble has peaked.  With the wide adoption of the internet at the time, the rapid growth of new startups looking to profit had never been higher.\nDo you feel a sense of déjà vu with current Artificial Intelligence?  You’re not the only one!  In 2025, 2000 to 3000 startup companies stated that their core business revolved around AI. There can be confusion around what AI actually is, but James Welch has put together this handy guide sheet .\nBut what does this all mean for the quality of AI output?\nAny AI that is powered by a Large Language Model (LLM) has the potential to generate completely plausible outputs that are factually incorrect or misinterpretations of their sources.  Some can be harmless fun with AI confidently telling a user that “The Golden Gate Bridge was transported for the second time across Egypt in October of 2016”, or it can begin causing more business-critical issues like Air Canada’s chatbot promising discounts to passengers that they’re not entitled to.\nThere are a wide range of reasons that this might happen, but the top 3 are grouped into the Crystal Ball Method, Training issues & Assumptions .\nCrystal ball method – LLMs, at their core, are not fact finders.  They’re built to analyse language and guess the next word based on patterns learned from massive data sets.  If the string of words makes sense together, then the system will output it.\nTraining issues – If you asked someone to write an essay on the solar system but only gave them material from 2005, they’d confidently say that Pluto was a planet.  However, if they were given up-to-date information, they’d know differently.  The same goes for LLMs.  If they’ve been given outdated information to train on, then what they can return will be outdated.\nAssumptions – When an AI is given a prompt like “Who is Alex Alexandra Alexson?”, based on training, the agent more often than not will assume that because you’re asking it a direct question about what it understands to be a name, that this person IS real and can feel “pressure” to generate a plausible response.\nThe effectiveness of AI models depends on two components: training data and model design. Training data serves as the foundation on which machine learning algorithms learn patterns, make predictions, and adapt to new situations. The quality, diversity, and volume of this data directly influence a model’s accuracy and reliability. High-quality datasets ensure that the AI captures meaningful relationships rather than listening to noise or biases, while diverse data helps prevent the model from underperforming in real-world scenarios that differ from the training environment. Poorly chosen or biased datasets can embed harmful stereotypes or inaccuracies into the model, highlighting the ethical importance of careful data selection and preprocessing.\nEqually important is model design, which covers the architecture, algorithmic approach, and optimisation strategies used to process the data. Model design determines how effectively an AI can recognise patterns from input data and transform them into useful outputs. For example, neural networks with deeper layers can capture complex, non-linear relationships, while simpler models may excel in efficiency and achieve a higher level of human understanding. Design choices also affect a model’s ability to generalise beyond its training data, its resilience to contradicting inputs, and its computational efficiency.\nEven the most sophisticated architecture cannot compensate for poor-quality data, and abundant high-quality data may be underutilised by a poorly designed model. Researchers and developers must approach AI development carefully, balancing rich, representative datasets with architectures tailored to the task at hand. Ultimately, this balance determines both the performance and ethical responsibility of AI systems, shaping their real-world impact across domains from healthcare to autonomous systems.\nNo one is immune to AI hallucinations.  From Alphabet (Google’s parent company) to legal representatives in court, issues are popping up in all industries.  I’ve pulled together a couple of examples of AI missing the mark when it comes to delivering reliable information.\nWhen Google’s AI Chatbot, Bard, provided factually incorrect information in a promotional video claiming that the James Webb Telescope was the first to take pictures of a planet outside our solar system.\nThis error caused concern from their investors that the company was falling behind rivals like OpenAI.  This caused the company to lose over $100 billion in market value, but they were quick to introduce a set of vetting guidelines to Bard’s answers to make sure that the information it was delivering was accurate, safe and based on fewer assumptions.\nThat’s right; Google’s overview feature hit the headlines for all the wrong reasons when it was released due to its, quite frankly, odd responses. One Google user was having trouble with the cheese staying on top of their pizza, to which the AI overview suggested adding a non-toxic glue with the sauce to make it more tacky !\nSome users believe that due to the prevalence of Reddit in Google’s SERPs, it was taking satirical content from the forum and mistaking it for fact. Although adding glue to your food is never advisable, the AI overview also brought back some more disturbing answers that Google was quick to step in and rectify.\nAlmost all industries have introduced AI Agents like ChatGPT into their day-to-day workflows to increase efficiency and cut down the mundane tasks, but when it comes to getting the facts wrong, it’s certainly not something you’d want from your legal representatives.\nWhile representing a man suing an airline for a routine personal injury suit, the lawyer prepared their filing using ChatGPT, which hallucinated during the process and delivered completely fake cases that the attorney presented in court without fact-checking them first.\nIt’s a well-known instance (loved as a feature in the nerdy side of the internet) that if you request it to generate an image of a random number, it will confidently create a prompt from it.\nFor instance, using the number 241543903 creates an image of a man with his head inside a fridge.  Why would this be?  In 2009, David Horvitz suggested that a user put their head inside the freezer to relieve a headache and posted a photo demonstrating this. The caption of the image was a combination of the serial number of the fridge and the barcodes for edamame and soba noodles he could see at the time, which came out to 241543903.\nSo, although it might seem to us that the number does not correlate with the image content, during learning, the LLM has paired the two together, showing that it might make connections rightly, or wrongly, that we might not think of straight away.\nThere are a lot of things that we can do to mitigate the impact of AI hallucinations.  Here are my top five ways that we can do this:\nThis is the core of all aspects of limiting the real-world impact of AI; They’re built to generate outputs based on patterns in data rather than actually verifying facts.  More often than not, hallucinations happen when the topic being asked is either rare with limited source material, an ambiguous prompt is given, or there is a request for up-to-the-minute information.\nChoosing the right AI for the right task can limit the chance of hallucinations.  It’s like deciding to take down a tree with a chainsaw as opposed to a plastic spoon.\nThis is the classic way to ensure that you don’t slip up like the lawyers who presented incorrect information before a judge; cross-check what the AI is outputting.  This method encourages users to use AI as a creative seed instead of the finished product. Instead of copying and pasting content from one window to another, use it as a leap pad; create your own ideas from it.\nWith the rise of AI startups, these are parallel with companies that are trying to keep AIs in check. I need to be clear here, there is no 100% solid solution, but there are systems such as Evidently AI that test, evaluate and observe machine learning.\nIn my personal opinion, this feels like the most sci-fi answer that I’ve got to the question “how can we hold AI Agents accountable for serving up incorrect information”.\nDon’t rely on just one agent; put your questions to a council of them. As humans, we gather information from multiple sources, cross-check with other subject experts and then take the agreed conclusion as the correct answer.  The LLM council aims to do the same.\nAlthough hallucinations can pose both funny and more serious issues, we need to understand that AI is a tool to be used for productivity and seeing, but not the voice of absolute truth.\nThere are a lot of great practical applications for AI that don’t allow them to be fully autonomous but are used to empower business functions to be more productive, look further into data that may take too long to sort through by hand and encourage confidence in users to ask questions and gain knowledge.\nAs of 2026, CES has had a large range of applications of AI suggested, including LG’s Signature Oven , which uses their Gourmet AI camera to detect when it is finished cooking.  This is a great example of understanding the limitations of AI; if your oven tells you a chicken is cooked but you can clearly see it isn’t, you’ll reject the AI’s output and instead choose the correct answer.  It has a suggestion, not the final say.\nAs AI has become the defining zeitgeist of our time, there is a nearly endless supply of content out on the internet talking about it more.  From applications of LLMs to the dangers of them, you can find content to suit whatever you’re in the mood for reading.  I would highly recommend these two pieces of content:\nYou might want to look at the bigger picture of Artificial Intelligence and consider why AI systems make these mistakes in the first place. The Netflix documentary Coded Bias explores how AI models inherit the assumptions, gaps, and blind spots of the data and people that shape them. AI hallucinations aren’t always just technical glitches; they’re a reminder that these systems don’t “know” the truth, only patterns. As AI becomes more embedded in real-world decisions, understanding its limitations is just as important as being impressed by its capabilities.\nLLM Council Paper – This goes into granular detail about how agents interact with each other, their bias on a council and the accuracy of their answers in a percentile format.  If you’re not a fan of reading the 50+ pages, there is an excellent YouTube video where Justin Zhao runs through the key points."
  }
]