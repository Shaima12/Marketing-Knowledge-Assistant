[
  {
    "url": "https://www.searchenginejournal.com/lets-be-honest-about-the-ranking-power-of-links/563662/",
    "title": "Let’s Be Honest About The Ranking Power Of Links via @sejournal, @martinibuster",
    "date": "2025-12-18T11:17:30+00:00",
    "content": "What link building should be trying to accomplish, in my opinion, is proving that a site is trustworthy and making sure the machine understands what topic your web pages fit into. The way to communicate trustworthiness is to be careful about what sites you obtain links from and to be super careful about what sites your site links out to.\nMaybe it doesn’t have to be said but I’ll say it: It’s important now more than ever that the page your link is on has relevant content on it and that the context for your link is an exact match for the page that’s being linked to.\nAlso make sure that the outgoing links are to legitimate sites, not to sites that are low quality or in problematic neighborhoods. If those kinds of links are anywhere on the site it’s best to consider the entire site poisoned and ignore it.\nThe reason I say to consider the site poisoned is the link distance ranking algorithm concept where inbound links tell a story about how trustworthy a site is. Low quality outbound links are a signal that something’s wrong with the site. It’s possible that a site like that will have its ability to pass PageRank removed.\nThis is how the Reduced Link Graph works, where the spammy sites are kicked out of the link graph and only the legit sites are kept for ranking purposes and link propagation. The link graph can be thought of as a map of the internet with websites connected to each other by links. When you kick out the spammy sites that’s called the reduced link graph.\nSearch engines are at a point where they can rank websites based on the content alone. Links still matter but the content itself is now the highest level ranking factor. I suspect that in general the link signal isn’t very healthy right now. Less people are blogging across all topics. Some topics have a healthy blogging ecosystem but in general there aren’t professors blogging about technology in the classroom and there aren’t HR executives sharing workplace insights and so on like there used to be ten or fifteen years ago.\nI’m of the opinion that links increasingly are useful for determining if a site is legit, high quality, and trustworthy, deeming it worthy for consideration in the search results. In order to stay in the SERPs it’s important to think about the outbound links on your site and the sites you obtain links from. Think in terms of reduced link graphs, with spammy sites stuck on the outside within their own spammy cliques and the non-spam on the inside within the trusted Reduced Link Graph.\nIn my opinion, you must be in the trusted Reduced Link Graph in order to stay in play.\nLink building is definitely not over. There’s still important. What needs to change is how links are acquired. The age of blasting out emails at scale are over. There aren’t enough legitimate websites to make that worthwhile. It’s better to be selective and targeted about which sites you get a (free) link from.\nSomething else that’s becoming increasingly important is citations, other sites talking about your site. An interesting thing right now is that sponsored articles, sometimes known as native advertising, will get cited in AI search engines, including Google AI Overviews and AI Mode. This is a great way to get a citation in a way that will not hurt your rankings as long as the sponsored article is clearly labeled as sponsored and the outbound links are nofollowed.\nLink building is still relevant, but not in the way it used to be. Its function now is likely more about establishing whether a site is legitimate and clearly associated with a real topic area, not to push rankings through volume, anchors, or scale. Focusing on clean outbound links, selective relationships with trusted sites, and credible citations keeps a site inside the trusted reduced link graph, which is the condition that allows strong content to compete and appear in both traditional search results and AI-driven search surfaces."
  },
  {
    "url": "https://www.searchenginejournal.com/how-will-ai-mode-impact-local-seo/561002/",
    "title": "How Will AI Mode Impact Local SEO? via @sejournal, @JRiddall",
    "date": "2025-12-18T11:00:49+00:00",
    "content": "In organic search, disruption has always been the norm, but the integration of AI into Google Search – with AI Overviews and now AI Mode – is not an incremental change; it is a fundamental restructuring. For marketers overseeing single or multi-location SEO strategies, the transition from the traditional blue-link environment to a conversational, synthesized search experience carries important stakes.\nThe initial manifestation of this shift, the AI Overview (AIO), which claims the premium “Position 0” real estate on a search engine results page (SERP), provided the initial shockwave. However, the long-term competitive reality is defined by AI Mode , a full conversational ecosystem where users can engage in multi-stage dialogue with AI. This interactive mode anticipates a user’s entire “information journey” by mapping out potential subsequent inquiries, known as latent questions or query fan-out, negating the need for users to click through for additional information.\nThe implications for local SEO are profound. Data confirms that when an AIO is present and a business’s content is not cited, organic click-through rates (CTR) can plummet by as much as 61% .\nThe priority for local marketing has irrevocably shifted: Success is no longer defined by securing Position 1 in the traditional organic listings, but by achieving inclusion and citation within the Position 0 AI Overview and the expanded AI Mode. Some are of the belief Google could go full AI Mode at any moment.\nThis blueprint outlines eight strategic imperatives for marketers to ensure resilient local visibility and drive high-intent conversions in the AI Mode era to come.\nThe mechanics of AI Mode fundamentally alter local search competition. For high-intent, local or transactional queries (e.g., “best walking tour in Chicago”), the AI often replaces the traditional Google 3-Pack with an expanded, enhanced local AI Mode display including Google Business Profile (GBP) cards.\nScreenshot from Google search for [best walking tours in New Orleans], November 2025 A limited study conducted in May 2025 found AI Overviews (now typically accompanied by AI Mode) appeared for local search queries 57% of the time and were particularly dominant for informational, as opposed to local/commercial, intent queries.\nA more recent behavioral study of travel booking in AI Mode found Google Business Profiles to be among the most highly displayed and engaged content for searchers booking local accommodations and experiences. This is likely the case for any locally oriented search. This creates new opportunities, but demands a strategic overhaul to ensure top-tier visibility.\nThe AI’s choice of businesses for this enhanced local pack leans heavily on Entity Authority. LLMs synthesize business summaries and attributes by drawing information from diverse, omni-channel sources. This reliance on verified, consistent facts across the entire web makes the digital ecosystem, rather than just the website’s content or backlink profile, the primary ranking vector.\nIn this new environment, traditional SEO and link acquisition strategies must be rebalanced with unique fact provision and entity authority strategies\nTo command a dominant position in the conversational search environment, local marketers must execute a comprehensive strategy focusing on local authority, data integrity, technical compliance, and an answer-first content structure.\nGBP has been identified as generative AI’s most critical source of verified local data . Full optimization and consistent verification are non-negotiable gatekeepers for inclusion and visibility within AI Mode.\nPrimary And Secondary Category Selection Choose the most relevant and appropriate primary category for the business, along with limited additional secondary categories. Do not select generic or non-relevant categories as a means to being included or found within the same via AI search. Far too many businesses make the mistake of choosing as many categories as they think are even tangentially related to the services they offer, often diluting their primary area of expertise.\nComprehensive Service Listings Ensure accurate and comprehensive listings of all services offered, aligning them perfectly with the services listed on the website and within schema markup. Here again, do not over-extend into generic or non-relevant service offerings.\nVerified Hours and Attributes Maintain current, verified hours of operation, paying special attention to temporary or seasonal closures. A newly important factor in organic and AI search visibility is whether or not a business is physically open when a search is being conducted.\nFill out all relevant business attributes, including payment types accepted, amenities (e.g., parking) available, and anything else which may set the business apart.\nActive Engagement Signals Behavioral signals, such as in-store visits tracked by Google Maps, and engagement signals on the GBP are increasing in importance, suggesting the AI weights profiles demonstrating real-world activity. Responding promptly to reviews and questions posed via GBP is critical, as is regularly posting photos, offers, updates, and other helpful content for your target audience.\nRecommendation: The GBP must be treated as a live, mission-critical data feed, not a static listing. Any change to a service, hour, or attribute must be propagated across the GBP first, then the website, and finally any other third-party local or industry-specific directories.\nStructured data can support AI search visibility. Large Language Models (LLMs), in part, use schema markup to categorize, verify, and ingest factual information directly. Failure to comply with stringent technical specifications may render an entity ineligible for expanded, visually-rich AI results.\nLocalBusiness Schema And Service Schema These must be implemented meticulously, defining the business type (e.g., Dentist, Vacation Rental Operator) and precisely describing the services offered using the Service and makesOffer properties.\nGeographical Precision The geo property (latitude and longitude) must be included in the LocalBusiness schema to satisfy the AI’s need for hyper-local accuracy in “near me” and navigational queries.\nVisual Asset Compliance To qualify for visually enhanced AI results, websites must provide multiple relevant service, product, and location-specific images. All images require relevant descriptive filenames and alt text, which must include pertinent keywords, where applicable.\nRecommendation: Implement all schema using JSON-LD for simplified maintenance and validation via Google’s Rich Results Test and Schema.org markup validator , keeping the technical markup separate from page design.\nGenerative AI systems rely on consistency and verifiability of a business’s factual data across multiple sources. Any conflict in Name, Address, and Phone (NAP) details, or service descriptions, across primary and third-party sources introduces ambiguity. AI models, like organic search algorithms preceding them, are programmed to reject or hesitate to cite conflicting data points, significantly degrading a business’s trustworthiness.\nGBP Vs. Website If a business lists four specific services on its website, but six on its Google Business Profile (GBP), the AI may not be able to provide a definitive, confident summary of service offerings.\nComprehensive Auditing Invest in robust, real-time auditing and monitoring tools to ensure 100% NAP consistency across the corporate website, all individual location pages, GBPs, and major third-party directories (e.g., Yelp, Tripadvisor).\nRecommendation: Treat your structured data and GBP as the single source of truth, and enforce a technical and content compliance mandate across all third-party listings and local data aggregators to eliminate signal dilution. Local authority is now synonymous with holistic entity management.\nWithin AI-search, Google continues to emphasize the E-E-A-T framework (Experience, Expertise, Authoritativeness, and Trustworthiness). For local entities, this can in part be demonstrated through verifiable user interactions, authentic customer feedback, and structured review data. The AI synthesizes customer reviews into concise, attribute-level summaries serving as the user’s immediate decision cue.\nAttribute-Level Prompting The strategy must shift from merely gathering high star ratings to encouraging customers to mention desirable operational attributes (e.g., “fast service,” “knowledgeable staff,” “great atmosphere”). This provides the AI with positive attributes to feature prominently in the generated summary, which acts as a primary conversion trigger.\nReview Schema Implementation Implementing Review and AggregateRating schema is critical for providing the AI model with a structured roadmap to quickly identify recurring sentiment themes.\nProactive Management Active, prompt management and response to both positive and negative reviews, focusing on service attributes, further establishes the ‘A’ authority and ‘T’ trust in E-E-A-T.\nContent strategy must transition from traditional keyword SEO to Answer Engine Optimization (AEO). AI Mode prioritizes highly informative, concise content specifically structured to answer user queries directly. Query fan-out refers to the process of not only answering the first query submitted, but also anticipating and providing answers to a range of subsequent related questions users have.\nMap Latent Questions Since complex queries often trigger AI Overviews, and AI Mode builds on the same multi-step reasoning systems, Google’s LLMs attempt to map the user’s broader information journey by predicting the follow-up questions they are likely to ask. Content therefore needs to address not only the initial ‘head query’ but also the latent questions that make up the next steps in that journey .\nStructure For Extraction Content inclusion is assessed partly by structure. Utilize clear formatting elements easy for the AI to extract and cite:\nThe AI’s holistic approach to entity authority means links are less important than they once were, while branded mentions are experiencing a resurgence. Research indicates a strong correlation between brands cited in AI Overviews/AI Mode and the frequency of their mention across the broader web (including social media, blogs, and forums like Reddit). In AI SEO, brand mentions (linked or not) are the new link . This shift is supported by data showing web mentions correlate highly with AI visibility.\nOmnichannel Entity Acquisition Proactively pursue high-quality, non-linked citations from authoritative local news sources, industry blogs, and high-quality directories. The goal is to maximize the sheer volume of high-quality, reinforcing brand mentions AI can reference.\nSocial & Video Integration Leverage social media platforms and, critically, YouTube content. LLMs scrape video and social channels for entity information and context, making these verifiable sources of service and brand attribute data.\nRecommendation: Shift resources from low-value link-building activities toward Digital PR and Content Distribution campaigns designed to earn non-linked brand mentions and reinforce local expertise across third-party industry and media sites.\nThe inevitable decline in raw organic traffic is accompanied by an efficiency challenge. The traffic successfully navigating from AI Mode to the website should typically be more qualified and higher-intent, as the AI has already satisfied low-intent informational needs. The traffic remaining is typically the commercially valuable “bottom-of-the-funnel” user.\nCRO Over Traffic Generation Resources should be strategically reallocated away from mass traffic generation toward maximizing the conversion potential of the qualified users who land on the website.\nOne interesting finding from the aforementioned AI Mode behavioral study was the number of users who expected to simply be able to complete their transaction once they left AI Mode, i.e., just click Book Now and pay. While this may be coming in the form of future Google integrations, the current transactional workflow requires users to start their booking from the beginning.\nWhile the percentage of traffic from AI search may initially be less than 1%, the potential volume – with 1% of a trillion searches equating to 10 billion opportunities – justifies a dedicated focus on conversion for this high-value segment.\nPerfecting Conversion Architecture The final click from AI Mode to the website must lead to a seamless, high-velocity user experience. This involves:\nA foundational requirement for AI Mode visibility is ensuring technical accessibility of content for the LLM’s consumption.\nUn-hide Critical Content Content crucial to establishing entity authority (e.g., licenses, certifications, key service attributes, location details) must not be hidden within toggles, tabs, accordions, or JavaScript requiring a user click to reveal.\nPlain Text And HTML While visuals are important, the core factual assertions must be rendered in clean, accessible HTML any machine can easily read and interpret.\nProactive Monitoring Use LLM analysis tools (or reverse question-answering prompts) to regularly audit which questions your site is answering and which critical facts are not being found by the AI, ensuring your core message is the stuff being crawled and indexed.\nGoogle AI Mode represents the definitive passing of the torch from traditional link-based SEO to a sophisticated strategy centered on fact provision and entity validation. For marketers, the shift is not one to debate, but one to embrace immediately.\nThe future of local search visibility is a high-stakes competition for the top-tier real estate of the AI Overview and AI Mode. The required investment is a mandate across the entire digital portfolio:\nThis strategic pivot – away from mass-traffic keyword pursuits and toward precise entity authority management – is the only way to mitigate the risk of CTR collapse and capitalize on the high-quality, high-intent traffic AI Mode will deliver. Your business must now be structured as an impeccable source of verified, structured facts for AI to cite. The time for strategic adaptation is now."
  },
  {
    "url": "https://www.searchenginejournal.com/google-says-what-to-tell-clients-who-want-seo-for-ai/563652/",
    "title": "Google Says What To Tell Clients Who Want SEO For AI via @sejournal, @martinibuster",
    "date": "2025-12-18T10:45:01+00:00",
    "content": "Google’s Danny Sullivan offered advice to SEOs who have clients asking for updates on what they’re going to do for AI SEO. He acknowledged it’s easier to give the advice than it is to have to actually tell clients, but he also said that advancements in content management systems drive technical SEO into the background, enabling SEOs and publishers to focus on the content.\nDanny Sullivan acknowledged that SEOs are in a tough spot with clients. He didn’t suggest specifics for how to rank better in AI search (although later in the podcast he did offer suggestions for what to do to rank better in AI search ).\nBut he did offer suggestions for what to tell clients.\n“And the other thing is, and I’ve seen a number of people remark on this, is this concern that, well, I’ve been doing SEO, but now I’m getting clients or people saying to me, but I need the new stuff. I need the new stuff. And I can’t just tell them it’s the same old stuff.\nSo I don’t know if you feel like you need to dress it up a bit more, but I think the way you dress it up is to say, These are continuing to be the things that are going to make you successful in the long-term. I get you want the fancy new type of thing, but the history is that the fancy new type of thing doesn’t always stick around if we go off and do these particular types of things…\nI’m keeping an eye on it, but right now, the best advice I can tell you when it comes to how we’re going to be successful with our AEO is that we continue on doing the stuff that we’ve been doing because that is what it’s built on.\nWhich is easy for me to say ’cause I don’t got someone banging on the door to say, Well, actually we do. And so we are doing that.\nSo that’s why, as part of the podcast, it’s just to kind of reassure that, look, just because the formats are changing didn’t mean you have to change everything that you had to do and that everything you had to shift around.”\nThere are many in the SEO community who are suggesting fairly spammy things to do to rank better in AI chatbots like ChatGPT, like creating listicles that recommend themselves as best whatever. Others are doing things like tweaking on keyword phrases, the kind of thing SEOs stopped doing by 2005 or 2006.\nThe problem with making dramatic changes to content in order to rank better in chatbots is that ChatGPT, Perplexity, and Anthropic Claude’s search traffic share is a fraction of a percent for each of them, with Claude close zero and ChatGPT estimated to be 0.2% – 0.5%.\nSo it absolutely makes zero sense to prioritize AEO/GEO over Google and Bing search at this point because the return on the investment is close to zero. It’s a different story when it comes to Google AI Overviews and AI Mode, but the underlying ranking systems for both AI interfaces remain Google’s classic search.\nDanny shared that focusing on things that are specific to AI risks complicating what should be simple.\n“And in fact, that the more that you dramatically shift things around, and start doing something completely different, or the more that you start thinking I need to do two different things, the more that you may be making things far more complicated, not necessarily successful in the long term as you think they are.”\nJohn Mueller followed up by mentioning that the advanced state of content management systems today means that SEOs and publishers no longer have to spend as much time on technical SEO issues because most CMS’s have the basics of SEO handled virtually out of the box. Danny Sullivan said that this frees up SEOs and creators to focus on their content, which he insisted will be helpful for ranking in AI search surfaces.\n“I think that makes a lot of sense. I think one of the things that perhaps throws SEOs off a little bit is that in the early days, there was a lot of almost like a technical transition where people initially had to do a lot of technical specific things to make their site even kind of accessible in search. And at some point nowadays, I think if you’re using a popular CMS like WordPress or Wix or any of them, basically you don’t have to worry about any of those technical details.\nSo it’s almost like that technical side of things is a lot less in the foreground now, and you can really focus on the content, and that’s really what users are looking for. So it’s like that, almost like a transition from technical to content side with regards to SEO.”\nThis echoed a previous statement from earlier in the podcast where Danny remarked on how some people have begun worrying less about SEO and focusing on content.\n“But we really just want you to focus on your content and not really worry about this. If your content is on the web and generally accessible as most people’s content is, that’s it.\nI’ve actually been heartened that I’ve seen a number of people saying things like: I don’t even want to think about this SEO stuff anymore. I’m just getting back into the joy of writing blogs.\nI’m like, yes, great. That’s what we want you to do. That’s where we think you’re going to find your most success.”"
  }
]