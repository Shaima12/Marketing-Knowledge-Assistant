[
  {
    "url": "https://www.searchenginejournal.com/eu-plan-to-simplify-gdpr-targets-ai-training-and-cookie-consent/561818/",
    "title": "EU Plan To Simplify GDPR Targets AI Training And Cookie Consent via @sejournal, @MattGSouthern",
    "date": "2025-11-24T15:22:42+00:00",
    "content": "The European Commission has proposed a “ Digital Omnibus ” package that would relax parts of the GDPR , the AI Act , and Europe’s cookie rules in the name of competitiveness and simplification.\nIf you work with EU traffic or rely on European data for analytics, advertising, or AI features, it’s worth tracking this proposal even though nothing has changed in law yet.\nThe Digital Omnibus would revise several laws at once.\nOn AI, the proposal would push back stricter rules for high-risk systems from August 2026 to December 2027. It would also lighten documentation and reporting obligations for some systems and move more oversight to the EU AI Office.\nRegarding data protection, the Commission aims to clarify when information is no longer considered ‘personal,’ making it easier to share and reuse anonymized and pseudonymized datasets, especially for AI training.\nPrivacy group noyb says this new wording isn’t just about clarifying the rules. They believe the proposal introduces a more subjective approach, hinging on what a controller claims it can or plans to do. Noyb warns this change could exclude parts of the adtech and data-broker industry from GDPR protections.\nThe cookie section is likely to be the most visible change for your day-to-day work if the proposal moves forward.\nThe Commission wants to cut “banner fatigue” by exempting some non-risk cookies from consent pop-ups and shifting more control into browser-level settings that apply across sites.\nIn practice, that would mean fewer consent banners for low-risk uses, such as certain analytics or strictly functional storage, once categories are defined.\nThe proposal would also require websites to respect standardized, machine-readable privacy signals from browsers when those standards exist.\nOne of the most contested pieces of the Digital Omnibus is how it treats data used to train AI systems.\nThe package would allow companies including Google, Meta, and OpenAI to use Europeans’ personal data to train AI models under a broadened legal basis.\nPrivacy groups have argued that this kind of training should rely on explicit opt-in consent, rather than the more flexible approach they see in the proposal.\nNoyb warns that long-running behavioral data, such as social media histories, could be used to train AI systems with only an opt-out model that is difficult for people to exercise in practice.\nThis proposal is worth keeping on your radar if you’re responsible for analytics, consent, or AI-driven products that reach EU users.\nOver time, you might observe smaller, browser-driven consent experiences for EU traffic, along with a different compliance approach for AI features that depend on behavioral data.\nFor now, nothing in your cookie banners, GA4 setup, or AI workflows needs to change solely because of the Digital Omnibus.\nThe Digital Omnibus is an early signal that the EU is re-balancing its digital rulebook around AI and competitiveness, not privacy and enforcement alone.\nKey items to monitor include Parliament’s amendments to AI training and data language, cookie and browser-signal provisions for CMPs and browsers, and changes to AI training and consent for EU users."
  },
  {
    "url": "https://martech.org/why-evergreen-content-expires-faster-in-an-ai-search-world-and-what-to-do-about-it/",
    "title": "Why evergreen content expires faster in an AI search world — and what to do about it",
    "date": "2025-11-24T14:01:00+00:00",
    "content": "Your best-performing article from last year just disappeared from ChatGPT’s results. The one that took three weeks to research, ranked in the top three for your core keyword and drove 40% of your demo requests last quarter.\nA competitor published something similar two weeks ago. Now their post shows up in AI-generated answers. Yours doesn’t exist. This isn’t because your content got worse. The definition of evergreen changed.\nAI search engines like ChatGPT, Perplexity and Gemini prioritize recency differently than traditional search. A comprehensive guide from 2023 loses to a solid update from last month. The content you built to rank for years now needs updates every few months to stay visible. If your content strategy assumes you can publish once and coast, you’re already behind.\nContent that once stayed relevant for 24–36 months now feels outdated in six to nine months. A marketing automation guide from 2022 may accurately cover core concepts, but it will likely overlook AI-driven workflows and the latest platform integrations.\nAn updated 2025 version includes those details. LLMs treat that version as more relevant, even if the older guide is longer. LLMs track market changes faster than traditional search engines. When freshness signals fade, your content loses ground to the LLMs and visibility drops.\nWhat to do: Treat every piece of content like it has a built-in decay timer. Assume a 90-day shelf life unless data proves otherwise. Add expiration dates to your content calendar. Schedule audits before content goes stale, not after traffic drops. A team publishing 10 new articles monthly needs bandwidth to refresh 10–15 existing pieces at the same rate. If that pace is unrealistic, publish less and focus on keeping your best assets current.\nDig deeper: Beyond the funnel: A new approach to content marketing\nLLMs evaluate freshness through a blend of technical, structural and external signals. Updating a date alone does little. The content needs to show signs of active upkeep, like these signals:\nFor instance, an email deliverability guide from 2023 gained visibility in Perplexity after the author added a section on 2025 authentication updates and published a substantial revision. The meaningful changes triggered new freshness signals and brought the piece back into synthetic answers.\nWhat to do: Build a checklist covering these signals. Update modified dates, add examples, expand sections, replace screenshots, revise FAQs, add links and update schema. Hit multiple signals in each refresh.\nA scalable refresh system requires two key components working together — a cadence that your team can sustain and the operational support that keeps it moving. Without both, refresh programs fail. Everything feels like a priority, the backlog grows and nothing moves. When foundational explainers get the same attention as high-converting assets, the workload becomes impossible to manage, leading to rushed updates or stalled initiatives.\nStart with the cadence. Tier content by strategic value so updates follow a predictable rhythm.\nThen, build the cadence directly into your content operations so that refreshes become a recurring production cycle. Assign clear owners, add refresh tasks to your project management tool and tie updates to specific dates and performance signals.\nTreat refreshes like sprints — not ad hoc work you pick up only after traffic drops. Schedule them the same way you plan new content and include re-promotion as part of the process so updated assets regain visibility in AI search.\nNext, identify which pieces need the most immediate attention. Watch for declining traffic over six months, dropping keyword rankings, competitors appearing in AI search or your content disappearing from ChatGPT, Perplexity or Gemini spot checks. These signals show where the cadence needs to focus first.\nWhen updating, make substantive changes. Bring in new data or updated stats, add recent examples, refresh screenshots and tool references, expand sections that cover emerging trends, update terminology and revise FAQs. Adjust intros to acknowledge recent developments so LLMs recognize the content as current.\nWhat to do: Stop treating refreshes as ad hoc projects. Build them into operations as recurring sprints. Tier your content, set up a 90-day calendar and assign ownership so refreshes become predictable.\nDig deeper: How to scale content without losing your brand voice\nSome brands dominate synthetic answers while others disappear because AI systems look for authority signals that show a source is reliable and worth citing. Below are some signals that give LLMs recurring reasons to include a brand in answers.\nLLMs also read patterns in your publishing history, which help AI systems recognize content that reflects real practice and reliable information.\nExample in action: A B2B SaaS brand jumped from zero citations to more than 15 ChatGPT appearances in six months after publishing quarterly benchmark reports, gaining press mentions and expanding a content cluster in its category.\nWhat to do: Pick three to five topical areas and build sustained authority there. Publish original research quarterly, pitch data to journalists and grow clusters of 20 or more interconnected pieces. Use named authors with real credentials.\nA practical system helps you move through refreshes efficiently and catch decay early.\nWhat to do: Choose one audit tool, one workflow tool and one automation habit to implement this week.\nDig deeper: How to optimize your website for AI-powered search\nThese habits kill your chances of staying visible in AI search. Cut them immediately.\nContent needs a repeatable path from publish to ongoing maintenance. This six-stage framework provides you with that path.\nWhat to do: Pull your top 10 assets and map where each sits in this lifecycle. Five might need immediate refreshes. Two might be ready to retire. Use this framework to diagnose what each asset needs.\nEvergreen content decays quickly in an AI-driven environment. Freshness comes from technical signals, structural updates and active participation in your category. Tiering your content keeps the workload manageable. Brand authority influences whether LLMs cite your content. Tools and templates make the process sustainable.\nWhat to do next: Open your analytics and list your top 20 URLs. Assign each one to a tier based on traffic and business value. Build a 90-day refresh calendar starting with Tier 1.\nContent that evolves stays visible. Content that stagnates fades. The teams that adopt this lifecycle now will gain the advantage as AI search becomes the primary way users discover information."
  }
]