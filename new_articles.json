[
  {
    "url": "https://www.searchenginejournal.com/user-data-is-important-in-googles-ranking-systems-liz-reids-appeal-declaration/565707/",
    "title": "User Data Is Important In Google Search, Per Liz Reid’s DOJ Filing via @sejournal, @marie_haynes",
    "date": "2026-01-23T15:00:37+00:00",
    "content": "I found some interesting things in the latest document in the DOJ vs. Google trial . Google has appealed the ruling that says they need to give proprietary information to competitors.\nThis really isn’t a surprise. I did find it interesting that freshness signals are at the heart of Google’s proprietary secrets.\nAgain, here’s more on the importance of Google’s proprietary freshness signals:\nEvery page in Google’s index is marked up with annotations to help it understand the page. These include signals to identify spam and duplicate pages. I’ve written before about how every page in the index has a spam score .\nGoogle doesn’t want to share information with its competitors on these scores.\nIf the spam scores get out, it could lead to more spamming and more difficulty for Google in fighting spam.\nThe pages that Google has added page understanding annotations on are organized based on how frequently Google expects the content will need to be accessed and how fresh the content needs to be.\nGoogle argues that giving competitors a list of indexed URLs will enable them to “forgo crawling and analyzing the larger web, and to instead focus their efforts on crawling only the fraction of pages Google has included in its index.” Building this index costs Google extensive time and money. They don’t want to give that away for free.\nThis is the most interesting part. I feel that we do not pay enough attention to Google’s use of user data. (Stay tuned to my YouTube channel as I’m soon about to release a very interesting video with my thoughts on how user-side data is so important – likely the MOST important factor in Google’s ranking systems.)\nGoogle Glue is a huge table of user activity . It collects the text of the queries searched, the user’s language, location and device type, and information on what appeared on the SERP, what the user clicked on or hovered over, how long they stayed on a SERP, and more.\nRankEmbed BERT is even more interesting. RankEmbed BERT is one of the deep learning systems that underpins Search. In the Pandu Nayak testimony, we learned that RankEmbed BERT is used in reranking the results returned by traditional ranking systems. RankEmbed BERT is trained on click and query data from actual users .\nThe AI systems behind search are continually learning to improve upon presenting searchers with satisfying results. Google looks at what they are clicking on and whether they return to the SERPs or not. Google also runs live experiments that look at what searchers choose to click on and stay on. Those actions help train RankEmbed BERT. It is further fine-tuned by ratings from the quality raters. I will be publishing more on this soon. The take-home point I want to hammer on is that user satisfaction is by far the most important thing we should be optimizing for!\nFrom the Liz Reid document we are analyzing today, we can see that user data is used to train, build, and operate RankEmbed models.\nOnce again, we learn that the user data that is used to train these models includes query, location, time of search, and how the user interacted with what was displayed to them.\nThis is talking about the actions that users take from within the Google Search results. What I really want to know is how much of a role Chrome data uses. Does Google look at whether people are engaging with your pages, filling out your forms, making your recipes, and more? I think they do. The judgment summary of this trial hints that Chrome data is used in the ranking systems, but not a lot of detail is shared.\nIt’s worthwhile reading the whole declaration from Liz Reid .\nThis post was originally published on Marie Haynes Consulting ."
  },
  {
    "url": "https://www.searchenginejournal.com/seo-pulse-googles-ai-mode-gets-personal-ai-bots-blocked-domains-matter-in-search/565681/",
    "title": "SEO Pulse: Google’s AI Mode Gets Personal, AI Bots Blocked, Domains Matter in Search via @sejournal, @MattGSouthern",
    "date": "2026-01-23T14:30:35+00:00",
    "content": "Welcome to the week’s SEO Pulse. This week’s updates affect how AI Mode personalizes answers, which AI bots can access your site, and why your domain choice still matters for search visibility.\nGoogle is rolling out Personal Intelligence , a feature that connects Gmail and Google Photos to AI Mode in Search, delivering personalized responses based on users’ own data.\nKey facts: The feature is available to Google AI Pro and AI Ultra subscribers who opt in. It launches as a Labs experiment for eligible users in the U.S. Google says it doesn’t train on users’ Gmail inbox or Photos library.\nThis is the personal context feature Google promised at I/O but delayed until now. We covered the delay in December when Nick Fox , Google’s SVP of Knowledge and Information, said the feature was “still to come” with no public timeline.\nFor the 75 million daily active users Fox reported in AI Mode, this could reduce how much context you need to type to get tailored responses. Google’s examples include trip recommendations that factor in hotel bookings from Gmail and past travel photos, or coat suggestions that account for preferred brands and upcoming travel weather.\nThe SEO effects depend on how this changes query patterns. If users rely on Google pulling context from their email and photos instead of typing it, queries may get shorter and more ambiguous. That makes it harder to target long-tail searches with explicit intent signals.\nThe early social reaction is framing this as Google pushing AI Mode from “ask and answer” into “already knows your context.” Robby Stein , VP of Product at Google Search, positioned it as a more personal search experience driven by opt-in data connections.\nOn LinkedIn, the discussion quickly moved to trust and privacy tradeoffs. Michele Curtis, a content marketing specialist, framed personalization as something that only works when trust comes first.\n“Personalization only works when trust is architected before intelligence.”\nSyed Shabih Haider, founder of Fluxxy AI, raised security concerns about connecting multiple apps.\n“Personal Intelligence.. yeah the features/benefits look amazing.. but cant help but wonder about the data security. Once all apps are connected, the risk for breach becomes extremely high..”\nRead our full coverage: Google Launches Personal Intelligence In AI Mode\nHostinger analyzed 66 billion bot requests across more than 5 million websites and found AI crawlers are following two different paths. Training bots are losing access as more sites block them. Search and assistant bots are expanding their reach.\nKey facts: Hostinger reports 55.67% coverage for GPTBot and 55.67% average coverage for OAI-SearchBot, but their trajectories differ. GPTBot, which collects training data, fell from 84% to 12% over the measurement period. OAI-SearchBot, which powers ChatGPT search, reached that average without the same decline. Googlebot maintained 72% coverage. Apple’s bot reached 24.33%.\nThe data confirms what we’ve tracked through multiple studies over the past year. BuzzStream found 79% of top news publishers block at least one training bot . Cloudflare’s Year in Review showed GPTBot, ClaudeBot, and CCBot had the highest number of full disallow directives. The Hostinger data puts numbers on the access gap between training and search crawlers.\nThe distinction matters because these bots serve different purposes. Training bots collect data to build models, while search bots retrieve content in real time when users ask questions. Blocking training bots opts you out of future model updates, and blocking search bots means you won’t appear when AI tools try to cite sources.\nAs a best practice, check your server logs to see what’s hitting your site, then make blocking decisions based on your goals.\nOn the practical SEO side, the most consistent advice is to separate “training” from “search and retrieval” in your robots decisions where you can. Aleyda Solís previously summarized the idea as blocking GPTBot while still allowing OAI-SearchBot, so your content can be surfaced in ChatGPT-style search experiences without being used for model training.\n“disallow the ‘GPTbot’ user-agent but allow ‘OAI-SearchBot'”\nAt the same time, developers and site operators keep emphasizing the cost side of bot traffic. In one r/webdev discussion , a commenter said AI bots made up 95% of requests before blocking and rate limiting.\n“95% of the requests to one of our websites was AI bots before I started blocking and rate limiting them”\nRead our full coverage: OpenAI Search Crawler Passes 55% Coverage In Hostinger Study\nGoogle’s John Mueller warned that free subdomain hosting services create SEO challenges even when publishers do everything else right. The advice came in response to a Reddit post from a publisher whose site shows up in Google but doesn’t appear in normal search results.\nKey facts: The publisher uses Digitalplat Domains, a free subdomain service on the Public Suffix List. Mueller explained that free subdomain services attract spam and low-effort content, making it harder for search engines to assess individual site quality. He recommended building direct traffic through promotion and community engagement rather than expecting search visibility first.\nMueller’s guidance fits a pattern we’ve covered over the years. Google’s Gary Illyes previously warned against cheap TLDs for the same reason. When a domain extension becomes overrun by spam, search engines may struggle to identify legitimate sites among the noise.\nFree subdomain hosting creates a specific version of this problem. While the Public Suffix List is meant to treat these subdomains as separate registrable units, the neighborhood signal can still matter. If most subdomains on a host contain spam, Google’s systems have to work harder to find yours.\nThis affects anyone considering free hosting as a way to test an idea before buying a real domain. The test environment itself becomes part of the evaluation. As Mueller wrote, “Being visible in popular search results is not the first step to becoming a useful & popular web presence.”\nFor anyone advising clients or building new projects, the domain investment is part of the SEO foundation. Starting on a free subdomain may save money upfront, but it adds friction to visibility that a proper domain avoids.\nMost of the social sharing here is treating Mueller’s “neighborhood” analogy as the headline takeaway. In the original Reddit exchange , he said publishing on free subdomain hosts can mean opening up shop among “problematic flatmates,” which makes it harder for search systems to understand your site’s value in context.\n“opening up shop on a site that’s filled with … potentially problematic ‘flatmates’.”\nOn LinkedIn, the story is being recirculated as a broader reminder that “cheap or free” hosting decisions can quietly cap performance even when everything else looks right. Fernando Paez V, a digital marketing specialist, called it out as a visibility issue tied to spam-heavy environments.\n“free subdomain hosting services … attract spam and make it more difficult for legitimate sites to gain visibility”\nRead our full coverage: Google’s Mueller: Free Subdomain Hosting Makes SEO Harder\nThis week’s stories share a common element. Access, whether to personal data, to websites via bots, or to fair evaluation by choosing the right domain, shapes outcomes before any optimization happens.\nPersonal Intelligence gives AI Mode access to your email and photos, changing what kinds of queries even need to happen. The Hostinger data shows search bots gaining access while training bots get locked out. Mueller’s subdomain warning reminds us that domain choice determines whether Google’s systems give your content a fair evaluation at all.\nThe common thread is that visibility increasingly depends on what you allow in and where you build. Blocking the wrong bots can reduce your chances of being surfaced or cited in AI tools. Building on a spam-heavy domain puts you at a disadvantage before you write a word. And Google’s AI features now have access to personal context that publishers can’t access or observe.\nFor practitioners, this means access decisions, both yours and the platforms’, shape results more than incremental optimization gains. Review your crawler permissions and domain choices, and watch how personal context in AI Mode changes the queries you’re trying to rank for."
  }
]