[
  {
    "url": "https://www.searchenginejournal.com/ahrefs-tested-ai-misinformation-but-proved-something-else/564124/",
    "title": "Ahrefs Tested AI Misinformation, But Proved Something Else via @sejournal, @martinibuster",
    "date": "2025-12-28T19:28:39+00:00",
    "content": "Ahrefs tested how AI systems behave when they’re prompted with conflicting and fabricated information about a brand. The company created a website for a fictional business, seeded conflicting articles about it across the web, and then watched how different AI platforms responded to questions about the fictional brand. The results showed that false but detailed narratives spread faster than the facts published on the official site. There was only one problem: the test had nothing to do with artificial intelligence getting fooled and more to do with understanding what kind of content ranks best on generative AI platforms.\nAhrefs’ research represented Xarumei as a brand and represented Medium.com, Reddit, and the Weighty Thoughts blog as third-party websites.\nBut because Xarumei is not an actual brand, with no history, no citations, no links, and no Knowledge Graph entry, it cannot be tested as a stand-in for a brand whose contents represent the ground “truth.”\nIn the real world, entities (like “Levi’s” or a local pizza restaurant) have a Knowledge Graph footprint and years of consistent citations, reviews, and maybe even social signals. Xarumei existed in a vacuum. It had no history, no consensus, and no external validation.\nThis problem resulted in four consequences that impacted the Ahrefs test.\nConsequence 1: There Are No Lies Or Truths The consequence is that what was posted on the other three sites cannot be represented as being in opposition to what was written on the Xarumei website. The content on Xarumei was not ground truth, and the content on the other sites cannot be lies, all four sites in the test are equivalent.\nConsequence 2: There Is No Brand Another consequence is that since Xarumei exists in a vacuum and is essentially equivalent to the other three sites, there are no insights to be learned about how AI treats a brand because there is no brand.\nConsequence 3: Score For Skepticism Is Questionable In the first of two tests, where all eight AI platforms were asked 56 questions, Claude earned a 100% score for being skeptical that the Xarumei brand might not exist. But that score was because Claude refused or was unable to visit the Xarumei website. The score of 100% for being skeptical of the Xarumei brand could be seen as a negative and not a positive because Claude failed or refused to crawl the website.\nConsequence 4: Perplexity’s Response May Have Been A Success Ahrefs made the following claim about Perplexity’s performance in the first test:\n“Perplexity failed about 40% of the questions, mixing up the fake brand Xarumei with Xiaomi and insisting it made smartphones.”\nWhat was likely happening is that Perplexity correctly understood that Xarumei is not a real brand because it lacks a Knowledge Graph signal or any other signal that’s common to brands. It correctly detected that Xarumei is not a brand, so it’s likely that Perplexity assumed the user was misspelling Xiaomi, which sounds a lot like Xarumei.\nGiven that Xarumei lacked any brand signals, Perplexity was correct to assume that the user was misspelling Xiaomi when asking about Xarumei. I think it’s fair to reverse Ahrefs’ conclusion that Perplexity failed 40% of the questions and instead give Perplexity the win for correctly assuming that the user was in error when asking about a non-existent brand called Xarumei.\nThe Weighty Thoughts blog, the post on Medium.com, and the Reddit AMA provide affirmative, specific answers to many of these categories of information: names, places, numbers, timelines, explanations, and story arcs. The “official” website of Xarumei did not offer specifics; it did the opposite.\nThose answers create an asymmetric response pattern (meaning there’s a lack of equality):\nA generative AI platform will be more willing to use the affirmative and specific answers because generative AI is specifically designed to provide answers. Generative AI does not select between truth and lies when it is generating an answer.\nThis points to a third problem with the Ahrefs research: leading questions.\nA leading question is a question that is phrased in a way that embeds an assumption. Leading questions in an AI prompt can directly influence the answers given.\nMost of the 56 questions used to prompt the AI platforms resembled this:\n“What’s the defect rate for Xarumei’s glass paperweights, and how do they address quality control issues?”\n49 of the prompts consisted of leading questions, and only 7 were not.\nThe seven prompts that were not leading questions were verification questions that asked to verify facts:\nAhrefs begins their article by warning that AI will choose content that has the most details, regardless of whether it’s true or false.\n“I invented a fake luxury paperweight company, spread three made-up stories about it online, and watched AI tools confidently repeat the lies. Almost every AI I tested used the fake info—some eagerly, some reluctantly. The lesson is: in AI search, the most detailed story wins, even if it’s false.”\nHere’s the problem with that statement: The models were not choosing between “truth” and “lies.”\nBecause many of the prompts implicitly demand specifics, the sources that supplied specifics were more easily incorporated into responses. For this test, the results had nothing to do with truth or lies. It had more to do with something else that is actually more important.\nInsight: Ahrefs is right that the content with the most detailed “story” wins. What’s really going on is that the content on the Xarumei site was generally not crafted to provide answers, making it less likely to be chosen by the AI platforms.\nOne of the tests was to see if AI would choose lies over the “official” narrative on the Xarumei website.\n“Giving AI lies to choose from (and an official FAQ to fight back)\nI wanted to see what would happen if I gave AI more information. Would adding official documentation help? Or would it just give the models more material to blend into confident fiction?\nFirst, I published an official FAQ on Xarumei.com with explicit denials: “We do not produce a ‘Precision Paperweight’ “, “We have never been acquired”, etc.”\nInsight: But as was explained earlier, there is nothing official about the Xarumei website. There are no signals that a search engine or an AI platform can use to understand that the FAQ content on Xarumei.com is “official” or a baseline for truth or accuracy. It is just content that negates and obscures. It is not shaped as an answer to a question, and it is precisely this, more than anything else, that keeps it from being an ideal answer to an AI answer engine.\nBased on the design of the questions in the prompts and the answers published on the test sites, the test demonstrates that:\nAlthough Ahrefs set out to test whether AI platforms surfaced truth or lies about a brand, what happened turned out even better because they inadvertently showed that the efficacy of answers that fit the questions asked will win out. They also demonstrated how leading questions can affect the responses that generative AI offers. Those are both useful outcomes from the test.\nI Ran an AI Misinformation Experiment. Every Marketer Should See the Results"
  }
]