{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aM-g3B1miRF"
   },
   "source": [
    "# **INSTALL LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlqxWpqCTutS",
    "outputId": "2c515d05-2952-442d-c36a-1a66db4d1a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langmem\n",
      "  Downloading langmem-0.0.30-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
      "Collecting langchain-anthropic>=0.3.3 (from langmem)\n",
      "  Downloading langchain_anthropic-1.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core>=0.3.46 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.1.0)\n",
      "Collecting langchain-openai>=0.3.1 (from langmem)\n",
      "  Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain>=0.3.15 in /usr/local/lib/python3.12/dist-packages (from langmem) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint>=2.0.12 in /usr/local/lib/python3.12/dist-packages (from langmem) (3.0.1)\n",
      "Requirement already satisfied: langsmith>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from langmem) (0.4.47)\n",
      "Collecting trustcall>=0.0.39 (from langmem)\n",
      "  Downloading trustcall-0.0.39-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.10)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
      "Collecting anthropic<1.0.0,>=0.73.0 (from langchain-anthropic>=0.3.3->langmem)\n",
      "  Downloading anthropic-0.75.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.46->langmem) (4.15.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.3.1->langmem) (2.8.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.3.1->langmem) (0.12.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint>=2.0.12->langmem) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.8->langmem) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
      "Collecting dydantic<1.0.0,>=0.0.8 (from trustcall>=0.0.39->langmem)\n",
      "  Downloading dydantic-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.73.0->langchain-anthropic>=0.3.3->langmem) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.73.0->langchain-anthropic>=0.3.3->langmem) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.73.0->langchain-anthropic>=0.3.3->langmem) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.73.0->langchain-anthropic>=0.3.3->langmem) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<1.0.0,>=0.73.0->langchain-anthropic>=0.3.3->langmem) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.46->langmem) (3.0.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.3.1->langmem) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.8->langmem) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.8->langmem) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=0.3.1->langmem) (2025.11.3)\n",
      "Downloading langmem-0.0.30-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_anthropic-1.2.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trustcall-0.0.39-py3-none-any.whl (30 kB)\n",
      "Downloading anthropic-0.75.0-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dydantic-0.0.8-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: dydantic, anthropic, langchain-openai, langchain-anthropic, trustcall, langmem\n",
      "Successfully installed anthropic-0.75.0 dydantic-0.0.8 langchain-anthropic-1.2.0 langchain-openai-1.1.0 langmem-0.0.30 trustcall-0.0.39\n"
     ]
    }
   ],
   "source": [
    "pip install langmem langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WPj1U_9wQdp",
    "outputId": "ccb047c3-dbc7-4861-cb3e-1e8aa83c1bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-1.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting groq<1.0.0,>=0.30.0 (from langchain_groq)\n",
      "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_groq) (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (0.4.47)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (9.1.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain_groq) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain_groq) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (2.5.0)\n",
      "Downloading langchain_groq-1.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
      "Successfully installed groq-0.37.1 langchain_groq-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UkzzuVvuk2n",
    "outputId": "ff8257b0-6ebc-40d3-b642-d1b3467437ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.12.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant-client) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmoWCYCMkEvZ",
    "outputId": "b14a8487-f96f-4d14-b56f-36eed70a7661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.6)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
      "Requirement already satisfied: python-magic in /usr/local/lib/python3.12/dist-packages (0.4.27)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.12.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client sentence-transformers pymupdf pillow pytesseract python-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP0iLC8PuloJ",
    "outputId": "420709fb-71f3-4537-c277-fb5855469143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
      "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
      "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.47)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openai qdrant-client langchain pypdf python-dotenv faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFGjV9nggidG",
    "outputId": "aa606979-4f04-4951-f0c1-3590af8b2ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitz in /usr/local/lib/python3.12/dist-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: configobj in /usr/local/lib/python3.12/dist-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in /usr/local/lib/python3.12/dist-packages (from fitz) (7.2.0)\n",
      "Requirement already satisfied: httplib2 in /usr/local/lib/python3.12/dist-packages (from fitz) (0.31.0)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from fitz) (5.3.2)\n",
      "Requirement already satisfied: nipype in /usr/local/lib/python3.12/dist-packages (from fitz) (1.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fitz) (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fitz) (2.2.2)\n",
      "Requirement already satisfied: pyxnat in /usr/local/lib/python3.12/dist-packages (from fitz) (1.6.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from fitz) (1.16.3)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2->fitz) (3.2.5)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel->fitz) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel->fitz) (4.15.0)\n",
      "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (8.3.1)\n",
      "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.6)\n",
      "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (2.1.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (7.5.0)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.20.2)\n",
      "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (7.0.2)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.20.0)\n",
      "Requirement already satisfied: acres in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (0.5.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: puremagic in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (1.30)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->fitz) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fitz) (2025.2)\n",
      "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (2.32.4)\n",
      "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.12/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->pyxnat->fitz) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S79FLvdWgwoa",
    "outputId": "a93593d5-3307-4ed8-a207-97677808ea84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlY4cPMngrbC",
    "outputId": "2808b6ad-8202-4a6f-e488-bfdfee397c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tools in /usr/local/lib/python3.12/dist-packages (1.0.15)\n"
     ]
    }
   ],
   "source": [
    "pip install tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_rQLogBrWIW",
    "outputId": "ffc20033-d1bf-4ca3-b350-e0ba405bfd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "\r",
      "0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.24)] [1 \r",
      "                                                                               \r",
      "Hit:2 https://cli.github.com/packages stable InRelease\n",
      "\r",
      "0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.24)] [1 \r",
      "0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.24)] [Co\r",
      "                                                                               \r",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "\r",
      "0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.24)] [Co\r",
      "                                                                               \r",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
      "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,201 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,842 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,901 kB]\n",
      "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,510 kB]\n",
      "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,281 kB]\n",
      "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,081 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,571 kB]\n",
      "Fetched 37.8 MB in 4s (8,631 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  poppler-utils\n",
      "0 upgraded, 1 newly installed, 0 to remove and 65 not upgraded.\n",
      "Need to get 186 kB of archives.\n",
      "After this operation, 697 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
      "Fetched 186 kB in 0s (458 kB/s)\n",
      "Selecting previously unselected package poppler-utils.\n",
      "(Reading database ... 121713 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "# Install Poppler for pdf2image\n",
    "!apt-get update\n",
    "!apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0W3mzTbRoktn",
    "outputId": "76f7f7c5-d121-4fbc-8044-dcbd916d5228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (1.1.0)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.4.47)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (2.12.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.11.12)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.3.1)\n",
      "Downloading langchain_huggingface-1.1.0-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: langchain_huggingface\n",
      "Successfully installed langchain_huggingface-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEGvPqHjo3_o",
    "outputId": "d9fccdf6-a490-412d-ba71-19676611c784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.1.0)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.47)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.11.12)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.41.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-classic, langchain_community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-text-splitters-1.0.0 langchain_community-0.4.1 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLiOu7t5qcfx",
    "outputId": "5d6d18fe-1850-4e1b-9c97-39b8607de819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pdf2image) (11.3.0)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pdf2image\n",
      "Successfully installed pdf2image-1.17.0\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZBtqS0cFQb"
   },
   "source": [
    "# **CHUNKING & METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52gFmI7mpaB5"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ✅ Imports\n",
    "# ================================\n",
    "import os\n",
    "import fitz\n",
    "import re\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVHgdJn8qlj7"
   },
   "outputs": [],
   "source": [
    "def semantic_chunk(text, similarity_threshold=0.65, max_len=1500):\n",
    "    sentences = re.split(r\"(?<=[.!?]) +\", text)\n",
    "    sentence_embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = util.cos_sim(sentence_embeddings[i], sentence_embeddings[i-1]).item()\n",
    "        if sim < similarity_threshold or sum(len(s) for s in current_chunk) > max_len:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "        else:\n",
    "            current_chunk.append(sentences[i])\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return [clean_text(c) for c in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMSY-Sjwoaqn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# =====================================\n",
    "# 📌 Extract category & topic correctly\n",
    "# =====================================\n",
    "def extract_category_topic(root_folder, current_path):\n",
    "    rel = os.path.relpath(current_path, root_folder)\n",
    "    parts = rel.split(os.sep)\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        return parts[0], None\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# ✅ Recursive Folder Indexer (UPDATED)\n",
    "# =====================================\n",
    "def index_folder(root_folder, branch_name):\n",
    "    documents = []\n",
    "\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            print(file_path)\n",
    "\n",
    "            # Extract category & topic from structure\n",
    "            category, topic = extract_category_topic(root_folder, root)\n",
    "            # ---------------------------------------\n",
    "            # PDF files → text + OCR + chunking\n",
    "            # ---------------------------------------\n",
    "            if ext == \".pdf\":\n",
    "                from langchain_community.document_loaders import PyPDFLoader\n",
    "                from pdf2image import convert_from_path\n",
    "                import pytesseract\n",
    "\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                pages = loader.load()\n",
    "\n",
    "                # Extract text from all pages\n",
    "                pdf_text = \" \".join(p.page_content for p in pages)\n",
    "\n",
    "                # OCR on images inside the PDF\n",
    "                try:\n",
    "                    images = convert_from_path(file_path)\n",
    "                    ocr_texts = []\n",
    "                    for img in images:\n",
    "                        txt = pytesseract.image_to_string(img)\n",
    "                        ocr_texts.append(txt)\n",
    "                    ocr_text = \" \".join(ocr_texts)\n",
    "                except Exception as e:\n",
    "                    print(f\"OCR failed for {file_path}: {e}\")\n",
    "                    ocr_text = \"\"\n",
    "\n",
    "                # Full combined text\n",
    "                combined_text = f\"{pdf_text} {ocr_text}\".strip()\n",
    "\n",
    "                # -------------------------------\n",
    "                # SPECIAL CASE: TEMPLATES (1 page)\n",
    "                # -------------------------------\n",
    "                if len(pages) == 1:\n",
    "                    chunks = [combined_text]\n",
    "                else:\n",
    "                    chunks = semantic_chunk(combined_text)\n",
    "\n",
    "                # Add as Document objects\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    metadata = {\n",
    "                        \"branch\": branch_name,\n",
    "                        \"category\": category,\n",
    "                        \"topic\": topic,\n",
    "                        \"file_name\": file,\n",
    "                        \"page_start\": 1,\n",
    "                        \"page_end\": len(pages),\n",
    "                        \"chunk_id\": f\"{os.path.splitext(file)[0]}_{i:03d}\"\n",
    "                    }\n",
    "                    documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "            # ---------------------------------------\n",
    "            # JSON files → one chunk per item\n",
    "            # ---------------------------------------\n",
    "            elif ext == \".json\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                if isinstance(data, list):\n",
    "                    for idx, item in enumerate(data):\n",
    "                        metadata = {\n",
    "                            \"branch\": branch_name,\n",
    "                            \"category\": category,\n",
    "                            \"file_name\": file,\n",
    "                            \"item_id\": item.get(\"id\", f\"{idx:03d}\"),\n",
    "                            \"chunk_id\": f\"{os.path.splitext(file)[0]}_{idx:03d}\"\n",
    "                        }\n",
    "                        documents.append(Document(page_content=json.dumps(item), metadata=metadata))\n",
    "\n",
    "                else:\n",
    "                    metadata = {\n",
    "                        \"branch\": branch_name,\n",
    "                        \"category\": category,\n",
    "                        \"file_name\": file,\n",
    "                        \"item_id\": \"item_001\",\n",
    "                        \"chunk_id\": f\"{os.path.splitext(file)[0]}_001\"\n",
    "                    }\n",
    "                    documents.append(Document(page_content=json.dumps(data), metadata=metadata))\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwO6aVTAq9hj"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ✅ Paths to both branches\n",
    "# ================================\n",
    "general_path = \"/content/drive/MyDrive/data/General data\"\n",
    "business_path = \"/content/drive/MyDrive/data/Buissnes data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvQO_OxY46en"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ✅ Index both branches\n",
    "# ================================\n",
    "import pickle\n",
    "general_docs = index_folder(general_path, \"general\")\n",
    "save_path = \"/content/drive/MyDrive/data/general_docs.pkl\"\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(general_docs, f)\n",
    "print(f\"{len(general_docs)} documents saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrWJuk2b445a"
   },
   "outputs": [],
   "source": [
    "business_docs = index_folder(business_path, \"business\")\n",
    "import pickle\n",
    "save_path = \"/content/drive/MyDrive/data/business_docs.pkl\"\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(business_docs, f)\n",
    "\n",
    "print(f\"{len(business_docs)} documents saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ut8K_EAkmJS"
   },
   "source": [
    "Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x44WoATIPy2"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = \"/content/drive/MyDrive/data/general_docs.pkl\"\n",
    "with open(save_path, \"rb\") as f:\n",
    "    general_docs = pickle.load(f)\n",
    "\n",
    "print(f\"{len(general_docs)} documents loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EC4qvZC1keRO"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = \"/content/drive/MyDrive/data/business_docs.pkl\"\n",
    "with open(save_path, \"rb\") as f:\n",
    "    business_docs = pickle.load(f)\n",
    "\n",
    "print(f\"{len(business_docs)} documents loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REsVlhBRGklC"
   },
   "outputs": [],
   "source": [
    "all_docs = general_docs + business_docs\n",
    "print(f\"Total documents indexed: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYfTC23shdon"
   },
   "source": [
    "# **EMBEDDING AND SAVE IN Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgp5DjShk4lG"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ✅ Cleaning function\n",
    "# ================================\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text for embedding: remove extra whitespace, newlines, non-ASCII, unicode artifacts, underscores\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = re.sub(r\"\\\\u[0-9a-fA-F]{4}\", \" \", text)  # Unicode escape sequences\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # multiple spaces -> one\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)  # remove non-ascii\n",
    "    text = re.sub(r\"[_]{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "def embed_documents(documents):\n",
    "    vectors = [embedding_model.encode(clean_text(doc.page_content)) for doc in documents]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493,
     "referenced_widgets": [
      "b1eed4d647904252a30bc3746c12f0ce",
      "1cc47b0a47874700a39a83773132e4d2",
      "6e7a9600d3744274979425a103f9fdb8",
      "524cfb64f7f14d988e7bc945d4566a2f",
      "6c1e4ecc0c4f4307ac5248406ade5c16",
      "f56a1e2c65774bfaa08d035de6dd9ed0",
      "8f55c1d6931c4cec83e8b86b77d9a122",
      "07299c864d5c4c73877ae29f60824140",
      "3e9f9aaf583749929bdd377594ce28db",
      "1f22874e2b9a47b5b995bc0edff857c7",
      "f8f3c4a158be49ce926ac6207fbd812a",
      "ff4c2720caaa478aaacdd07e5846b8ba",
      "6a8a68feb7e348f692a2faa0adbf4333",
      "3c432aa796aa4029838a9ac1e69f6f0b",
      "bbee4ce12ca84912829b317892b966f6",
      "0322eb97b3cb452985adc3ac62220784",
      "702de46cf47e4959a42b0fefa5bb129f",
      "bde6fe64b55448df9e2f301d9a9a1097",
      "6cfdd61e02a144759532e710f5612ee7",
      "5a555152964d4c97a04a9cee3d355ad8",
      "830a7711e8614460825651f05a1bc521",
      "5e23b4943ab1441ba16e48c7127d43c1",
      "700dfd9b5c364b5286b0033126c3ea13",
      "94f4ff4478254d5e8290367fd0b5049f",
      "b4711c9a5f7b4712b389fb626d4565fc",
      "2c57cde84521443fae2bc129959d2ed6",
      "0865379113ec456aa8a61344f972a41c",
      "72edef899b644b21a2392dc25b66d780",
      "aeae1ff98ab743409130bc35a9f17a4f",
      "6b856a513dd848aabc78d18fb23e5637",
      "9f184866d0b0421bac9c27ce32df9243",
      "b96056b457504f2d9c93b2692ac72377",
      "86e72ec3695846398dc78a3e02cf3f2b",
      "0191ed1bc91746d6a76ac9c601e5a5d1",
      "f3481fbac8b2426f82ffd3ed449c1890",
      "bb55d5a30b74481f9672869c2d188af4",
      "096ed899e6854b4f87ff708789599c0f",
      "55b19ca1872849a49beb3de2855127d7",
      "b620b991fef6407da2d15584f6dfeee7",
      "16acb37199454390b3d031caa7d277d7",
      "07f121c146014d23bd48ba00d5a993b1",
      "8da1a2f8c5144b51ab37a688727641d0",
      "90573b586f73499b98e69f38127c886c",
      "c89e7e484f604168b59eed39c2eddfc9",
      "4e7c9d966f864cfbaa8afa51faf6f459",
      "59b119c9c23746c8bcc2fc8885be58a4",
      "4e196d2461dd4c5ab27acc1387967032",
      "57c7344ad692421090077bbb8e7a2b30",
      "5daf1807321b4eb3952135c9d1c3bcde",
      "349fc27b6a8843b0b8a01bd9f5442561",
      "82cd1f1d0bd9419e95fd14f626ceb1a7",
      "930d75a6ead94fdf9b860c68ad497557",
      "2e0183a611c24aafb03a46cadbd5eb55",
      "66860e6c411f47b5bba26a0cfaa43fb1",
      "052286849cdf4cafbe649c3b21a5a1d7",
      "16c01639d9464d219414813c402dbcbe",
      "bb98e33a763c421eac16950c1509d9c1",
      "b3812b4fb80e428d8762255f6d226985",
      "7edb8b22940d49ee987337f72c33359c",
      "aacc51015ee7494884f300c3ecd380c9",
      "99fb6116f5b9487c994e1671963de4a6",
      "cb9ddcf5338f449bbad9e8724e9b47b3",
      "51f3ded9246e4496bc23ff41ce3fdd77",
      "a500594c0067468fbe6c239263f2b5b3",
      "ef7b4ec4eb954e798cd92b26584c2a88",
      "ac33ea89013b43b0ba0a65713af38aab",
      "585f6099ce714edc9325f5af7c5e5ce6",
      "95b78858dd344d0b884e2ccc921de8c1",
      "2facc398a82f4c0caa7d4a7ced1b3197",
      "8d6f72fc40ff4d22bdf2d1828648048c",
      "b6bc753b00314425924924da22fd6543",
      "19d994eb455a41e9ba14a921faea17d0",
      "8d1da2fd3e2c47b898352d14c21aff95",
      "2af49c3ecf0e406c98b88eb2d17ff1ad",
      "08105d0af8d9481da879a1923b6fd49a",
      "bd8a208546f843e9aeb8cc3da677e66e",
      "2f68fcb2ab69456fbc8033ca717bfbc2",
      "6b109590fb35410d8a52fced9c9481ce",
      "49ae83ed32b745e187627e7ff3e45ded",
      "917f09d549844560872047d6ccecee67",
      "aea0c57b468348aebbd206656acbd6a7",
      "79efbfcac6ad4d39ab48fc15807f3206",
      "5b8cb54591694c479e65a24bcfcedfeb",
      "706b02749144436f90c27cba43740f0a",
      "b4556d54f85046dc997b1136d15be345",
      "b412c5a9382f441aa79efb6e50ee929c",
      "6b7d1f7efcf94f93aeec714ee51f3a8b",
      "8a3d714dec594ed89850a3906dfffc6f",
      "4d12bd47de0840a792438f263c5d753b",
      "91be4b72a3fa4037b9461198f3d36458",
      "d372344e807c466c8db12e56e07ce062",
      "9c8f88afdce84102adbeb3096d142fe4",
      "d28c4429f4fe43a693c8ced3f74c3541",
      "df49766385f647a6aca97b35db3baccb",
      "abb5d9c768ea4e92995a13b43d685311",
      "7574b30cad7c4a9aab0617dd9f31e63b",
      "08233ca8b7b14aab9cade6d2927d64cf",
      "6f3af7d5ec334371822030875dd165b9",
      "06430fee37584dc785361f8679a37da2",
      "4eeaa63053434baab8d88aa236162300",
      "262000878f434587be78166dbd9e1207",
      "b15865984e4c483c994642d2039d19da",
      "2156c01fa3334746bf9f25454a61b090",
      "15662dd4471f447b9251d3269baf649e",
      "6d6165ec35334beca70cc4314118fe3b",
      "5bac49e20a8e4a5682c45ac89ce26e33",
      "b83c547793b14be2a0daa9037b02605a",
      "501b98c324cc4ae2b3ea00aa96da3ef3",
      "2cb9a786cf3c442fa8b66b569175fbd0",
      "d57551c567754dcabdd17d86e5f078de",
      "72d28395448842378b28646b748e9b56",
      "5db866ddf34b4008a9157670f7af7024",
      "5988dc46f1df4e3c9f10ea4880e5dc36",
      "f88bfd354d464dd8b5ef675336fa99d0",
      "7a5b23fd54bc4d3480e73db1d34e9163",
      "a65c03af34c54e51bee19a4368b284f9",
      "2aafdaa453454b2cb51e6e2b7183bee0",
      "3c0874d6618949239988e38661f80a5b",
      "b77edae748e1499fa7ac609b444230bb",
      "08e200f3bf2c461a98d67552782291f3",
      "d9bdcc935a3a44d3a103c5ce1e248552"
     ]
    },
    "id": "A7VVVXxbogK7",
    "outputId": "15388818-ab02-44cb-d5ff-2347bb0ea337"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1eed4d647904252a30bc3746c12f0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4c2720caaa478aaacdd07e5846b8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700dfd9b5c364b5286b0033126c3ea13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0191ed1bc91746d6a76ac9c601e5a5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7c9d966f864cfbaa8afa51faf6f459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c01639d9464d219414813c402dbcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585f6099ce714edc9325f5af7c5e5ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b109590fb35410d8a52fced9c9481ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d12bd47de0840a792438f263c5d753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeaa63053434baab8d88aa236162300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d28395448842378b28646b748e9b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# ================================\n",
    "# ✅ Semantic chunking function for PDFs\n",
    "# ================================\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnfhtmZko9Gp"
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "client = QdrantClient(\n",
    "    url=\"https://d7bb08f9-84c8-4901-b9ea-c30ad9c70822.europe-west3-0.gcp.cloud.qdrant.io:6333\",  # your cluster URL\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.VFLaw0AblT3k3pKyICEV0JdXmBx-y_YJacDqT5Belz0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgohFJ9Zhj7N"
   },
   "outputs": [],
   "source": [
    "vector_size_general = general_vectors[0].shape[0]\n",
    "vector_size_business = business_vectors[0].shape[0]\n",
    "\n",
    "# 2️⃣ Create collections (recreate if already exists)\n",
    "client.recreate_collection(\n",
    "    collection_name=\"general_docs\",\n",
    "    vectors_config=VectorParams(size=vector_size_general, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"business_docs\",\n",
    "    vectors_config=VectorParams(size=vector_size_business, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 3️⃣ Prepare PointStructs\n",
    "general_points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=general_vectors[i].tolist(),\n",
    "        payload=general_docs[i].metadata\n",
    "    )\n",
    "    for i in range(len(general_docs))\n",
    "]\n",
    "\n",
    "business_points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=business_vectors[i].tolist(),\n",
    "        payload=business_docs[i].metadata\n",
    "    )\n",
    "    for i in range(len(business_docs))\n",
    "]\n",
    "\n",
    "# 4️⃣ Upsert points into collections\n",
    "client.upsert(collection_name=\"general_docs\", points=general_points)\n",
    "client.upsert(collection_name=\"business_docs\", points=business_points)\n",
    "\n",
    "print(\"✅ Indexing & embedding complete for both branches!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkryGBEPgnDl"
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "# Votre client Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    url=\"https://d7bb08f9-84c8-4901-b9ea-c30ad9c70822.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.VFLaw0AblT3k3pKyICEV0JdXmBx-y_YJacDqT5Belz0\"\n",
    ")\n",
    "\n",
    "print(\"✅ Client Qdrant Cloud connecté\")\n",
    "\n",
    "# ================================\n",
    "# ✅ Ré-indexer AVEC LE CONTENU\n",
    "# ================================\n",
    "\n",
    "# 1️⃣ Embed documents (vous l'avez déjà fait)\n",
    "print(\"\\n🔄 Embedding des documents...\")\n",
    "general_vectors = embed_documents(general_docs)\n",
    "business_vectors = embed_documents(business_docs)\n",
    "\n",
    "vector_size_general = general_vectors[0].shape[0]\n",
    "vector_size_business = business_vectors[0].shape[0]\n",
    "\n",
    "print(f\"✅ Vecteurs créés:\")\n",
    "print(f\"  - General: {len(general_vectors)} documents\")\n",
    "print(f\"  - Business: {len(business_vectors)} documents\")\n",
    "\n",
    "# 2️⃣ Supprimer et recréer les collections\n",
    "print(\"\\n🔄 Recréation des collections...\")\n",
    "\n",
    "# Supprimer anciennes collections\n",
    "try:\n",
    "    client.delete_collection(\"general_docs\")\n",
    "    print(\"  ✓ Ancienne collection general_docs supprimée\")\n",
    "except Exception as e:\n",
    "    print(f\"  - general_docs n'existait pas: {e}\")\n",
    "\n",
    "try:\n",
    "    client.delete_collection(\"business_docs\")\n",
    "    print(\"  ✓ Ancienne collection business_docs supprimée\")\n",
    "except Exception as e:\n",
    "    print(f\"  - business_docs n'existait pas: {e}\")\n",
    "\n",
    "# Créer nouvelles collections\n",
    "client.create_collection(\n",
    "    collection_name=\"general_docs\",\n",
    "    vectors_config=VectorParams(size=vector_size_general, distance=Distance.COSINE)\n",
    ")\n",
    "print(\"  ✓ Collection general_docs créée\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"business_docs\",\n",
    "    vectors_config=VectorParams(size=vector_size_business, distance=Distance.COSINE)\n",
    ")\n",
    "print(\"  ✓ Collection business_docs créée\")\n",
    "\n",
    "# 3️⃣ Préparer les points AVEC LE TEXTE\n",
    "print(\"\\n🔄 Préparation des points avec contenu texte...\")\n",
    "\n",
    "general_points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=general_vectors[i].tolist(),\n",
    "        payload={\n",
    "            **general_docs[i].metadata,              # Métadonnées\n",
    "            \"text\": general_docs[i].page_content     # ← CONTENU TEXTE!\n",
    "        }\n",
    "    )\n",
    "    for i in range(len(general_docs))\n",
    "]\n",
    "\n",
    "business_points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=business_vectors[i].tolist(),\n",
    "        payload={\n",
    "            **business_docs[i].metadata,             # Métadonnées\n",
    "            \"text\": business_docs[i].page_content    # ← CONTENU TEXTE!\n",
    "        }\n",
    "    )\n",
    "    for i in range(len(business_docs))\n",
    "]\n",
    "\n",
    "print(f\"  ✓ {len(general_points)} points general préparés\")\n",
    "print(f\"  ✓ {len(business_points)} points business préparés\")\n",
    "\n",
    "# 4️⃣ Upload en batch (pour éviter timeout)\n",
    "print(\"\\n⏳ Upload dans Qdrant Cloud...\")\n",
    "\n",
    "# Upload general docs par batch de 100\n",
    "batch_size = 100\n",
    "for i in range(0, len(general_points), batch_size):\n",
    "    batch = general_points[i:i+batch_size]\n",
    "    client.upsert(collection_name=\"general_docs\", points=batch)\n",
    "    print(f\"  ✓ General batch {i//batch_size + 1}/{(len(general_points)-1)//batch_size + 1} uploadé\")\n",
    "\n",
    "# Upload business docs\n",
    "for i in range(0, len(business_points), batch_size):\n",
    "    batch = business_points[i:i+batch_size]\n",
    "    client.upsert(collection_name=\"business_docs\", points=batch)\n",
    "    print(f\"  ✓ Business batch {i//batch_size + 1}/{(len(business_points)-1)//batch_size + 1} uploadé\")\n",
    "\n",
    "print(\"\\n✅ Upload terminé!\")\n",
    "\n",
    "# 5️⃣ Vérification complète\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 VÉRIFICATION FINALE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for collection_name in [\"general_docs\", \"business_docs\"]:\n",
    "    # Compter les points\n",
    "    count = client.count(collection_name=collection_name).count\n",
    "\n",
    "    # Récupérer un exemple\n",
    "    scroll_result = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        limit=1,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    points = scroll_result[0]\n",
    "\n",
    "    print(f\"\\n📁 {collection_name}:\")\n",
    "    print(f\"  Total points: {count}\")\n",
    "\n",
    "    if points:\n",
    "        point = points[0]\n",
    "        print(f\"  Champs disponibles: {list(point.payload.keys())}\")\n",
    "\n",
    "        has_text = 'text' in point.payload\n",
    "        print(f\"  {'✅' if has_text else '❌'} Contient 'text': {has_text}\")\n",
    "\n",
    "        if has_text:\n",
    "            text = point.payload['text']\n",
    "            print(f\"  Longueur du texte: {len(text)} caractères\")\n",
    "            print(f\"  Aperçu: {text[:200]}...\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ ERREUR: Pas de champ 'text' trouvé!\")\n",
    "            print(f\"  Payload complet: {point.payload}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ INDEXATION TERMINÉE AVEC SUCCÈS!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRcSFq1f1DTz"
   },
   "source": [
    "test from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca5YmLxrppgt"
   },
   "outputs": [],
   "source": [
    "collections = client.get_collections()\n",
    "print(collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5ieFjZtsrB1"
   },
   "outputs": [],
   "source": [
    "general_info = client.get_collection(collection_name=\"general_docs\")\n",
    "business_info = client.get_collection(collection_name=\"business_docs\")\n",
    "general_count = client.count(collection_name=\"general_docs\").count\n",
    "business_count = client.count(collection_name=\"business_docs\").count\n",
    "\n",
    "print(\"General docs count:\", general_count)\n",
    "print(\"Business docs count:\", business_count)\n",
    "\n",
    "general_points = client.scroll(collection_name=\"general_docs\", limit=3)\n",
    "business_points = client.scroll(collection_name=\"business_docs\", limit=3)\n",
    "\n",
    "print(\"Sample general points:\", general_points)\n",
    "print(\"Sample business points:\", business_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu8I6HSnu3r2"
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "query= \"Write Tiktok post for my product Luna Denim Jacket\"\n",
    "query_vec = model.encode(query).tolist()\n",
    "\n",
    "results = client.query_points(\n",
    "   collection_name=\"business_docs\",\n",
    "   query=query_vec,\n",
    "   limit=5,\n",
    "   with_payload=True\n",
    ")\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ64Y14bmVuB"
   },
   "source": [
    "# **REWRITE THE QUERY WITH LLMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "e24b9b047dca46759bf8c23bf0d6b6cc",
      "fa17bded730d440fa04f38f11c6a8423",
      "cc167a9f58b04c0bb60062b7ac324875",
      "002471fbb62b4507b05777aa29e730a3",
      "7d9c1c54fb314fb2bb5fc5d83a26f528",
      "c2752d21fa9c4196827bb1e155ef58a0",
      "b26e6328421644328e72b8a9f34ee383",
      "e665368716774854ac13cbba0a80ef68",
      "89eec28c0c724982a97ac5c9430d66bf",
      "30deb3c2b3594763b97b30c851986eac",
      "a6693190b8e74e5d9de562c78c301fbb",
      "1948d8d5b7704854b9f7cef93e5e59b0",
      "3e90ca35aba148f0bef21c0a624b5bdc",
      "39ecfcefe45d459689b5660319a95fdd",
      "a70d5ac5f3d6489289864b35e420f587",
      "bb199a01bab342ed99aefe833f4811de",
      "858b28e15dd64481ade617510def10dd",
      "dcd968ab81494a4ab94e3642e93f888a",
      "7bc92f99e5ad432788b28bcfce254318",
      "b6df38661e80487aa616e2c5a068e595",
      "b1b3c8b7919f4289909cf065af64a4cc",
      "5969843bf86d4df08a4109321024de11",
      "6d93b1073a754a15a1a630b32ec524e1",
      "a7b728c5282747658c1f0708106d6b74",
      "bd7550fdb9b147d09857b0b4e8d05fab",
      "e5a542c8883149eaa7a7f14f168b724e",
      "c377a52b08064895a1860fcdbeb0042a",
      "d8f48094ce5c4a2d85c6891410a717a2",
      "6d1c48e982b048d09671983990a4b05c",
      "bcfc9ec2bbd64cfabadb783c3efabb4e",
      "1a7dc05b09a24f4c89193d25ee1b9e90",
      "5754b8623efa424c8ad359b70e2a444b",
      "50c688c173f74260bb362f71815c15c9",
      "ddd2eb8d024f4792a31a8413aec0bfac",
      "b4a2ad12714842b9ae22a8cd13b173b9",
      "05cf6b1487a9486f8d3f025ba8b75eb0",
      "11243052e49c4ad8a9768a15be8eab95",
      "a2aae45d23674a39910f269ad6265fff",
      "5fc7ffad58f745d5ae0e5065e8641130",
      "be90a049c7434d078605efa33b15c64b",
      "9d85f2d865d94062ae467c867413a181",
      "d878330dbe8044d68b8f8b46ebeaf2de",
      "7455bc91e89f4be1aaee4322ac528e2a",
      "b0d6fb4d7ac8483daa25dba856cf4fa2",
      "a3cf47b91dd7410088f10fa9f0f7e0fd",
      "9b2ccf6dbe314efaa480d80fe802db08",
      "1c32691f06f140fb94cee601245a69c0",
      "49892bea3eba40f9b6cfabcdd7d5944b",
      "47d2cdfa277440cfbcfb2a5de4ae64d9",
      "3828789e90e945889038d58f277303b5",
      "e78a5d44e0e64323b1cd9b788ecce451",
      "64b2c429da9f48758f5b434c9a7a9338",
      "62b9a30985c144c18502e4919200de44",
      "a0f54e73c8044bdba9496846472db256",
      "35eb22aab1ee49859f89a7db6386bb4c",
      "a67114652aa245eea91e7d1a3b4f6868",
      "d170b2db601a48c0b26ddf2e757137f1",
      "2751fb337c6b4d969e9d06e3aa16495d",
      "cd1c4313f80b437daecd75ac00f5c887",
      "42e3cc522b4a4abdb557d0f590e55655",
      "cf7305a7d1e94fe8bd19ef9cf7d27784",
      "07fae29d49504fd1b83e7dcbb8108009",
      "48fc50bd80df46cb8c4434e3b864e26b",
      "29417d40aea542f7ae59532ef001fc56",
      "fd1ae7d13a464862a7fcf3d1bd04c211",
      "4ea4f87a60a04db287485d6246999878",
      "ef6a74d1f98743e1b33b0953d6bd468a",
      "54cd9952b71545369fd332901c76313a",
      "49bf1cd68c6f45928ce80bc24157a2cc",
      "cbf3dbd95b324b8f8314e1dd39710b71",
      "f6dde666aed0469f9badecc1168c890b",
      "6035ac29c4e44e7cacf24af64e66947c",
      "7b3db2e270a64cac91216bfe645a0680",
      "b7e7b9bcf8ae45ab8831d6b45b1672fa",
      "fd2044da26b849d2adb1d10b7928764c",
      "2e689950feca4cdab866556935cc762a",
      "c03a0f7d30824991b88fed31313802bc"
     ]
    },
    "id": "8I9pRRPK05N_",
    "outputId": "f95b3e13-0ab1-4046-9c17-2f049159bfab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24b9b047dca46759bf8c23bf0d6b6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1948d8d5b7704854b9f7cef93e5e59b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d93b1073a754a15a1a630b32ec524e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd2eb8d024f4792a31a8413aec0bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cf47b91dd7410088f10fa9f0f7e0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67114652aa245eea91e7d1a3b4f6868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6a74d1f98743e1b33b0953d6bd468a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4itqKM_5fvUX"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_first_json(response_text):\n",
    "    \"\"\"\n",
    "    Extrait le premier objet JSON trouvé dans une réponse texte.\n",
    "    \"\"\"\n",
    "    json_block_match = re.search(r\"```json\\s*(.*?)\\s*```\", response_text, re.DOTALL)\n",
    "    if json_block_match:\n",
    "        json_string = json_block_match.group(1)\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"⚠️ Erreur JSON (bloc markdown):\", e)\n",
    "            print(\"Texte JSON brut:\", json_string)\n",
    "\n",
    "    match = re.search(r\"\\{.*?\\}\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"⚠️ Erreur JSON (general search):\", e)\n",
    "            print(\"Texte JSON brut:\", match.group())\n",
    "            return None\n",
    "    else:\n",
    "        print(\"⚠️ Aucun JSON trouvé dans la réponse.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLjo3YmfmcBF"
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# ================================\n",
    "# 🧠 LLM 1: QUERY DECOMPOSITION\n",
    "# ================================\n",
    "def decompose_query(original_query):\n",
    "\n",
    "    decomposition_prompt = f\"\"\"\n",
    "You are a QUERY ROUTER for a multi-database RAG system.first you should understand the intent of the query and what we need from general data and what we need from buisness data to answer the user query .\n",
    "and you should answer with one json answer.\n",
    "DATABASES:\n",
    "\n",
    "GENERAL_DOCS contains:\n",
    "- Marketing templates,Social media strategies,TikTok, Instagram, YouTube content informations,Brand positioning theory\n",
    "\n",
    "BUSINESS_DOCS contains:\n",
    "- Products (name, description, materials, price, category, supplier) , Brand tone and values ,the objectives,KPIs ,Customer profiles,Customer questions and Customerreviews\n",
    "\n",
    "YOUR TASK:\n",
    "You MUST always analyse and decompose the query into two search queries:\n",
    "1) ONE query for GENERAL_DOCS (marketing knowledge, templates, strategy)->a detailed query ask question what best techniques what best strategies what the methodes to answer the user query\n",
    "2) ONE query for BUSINESS_DOCS (product, brand, customer, KPIs)->extract from business data  the elements to answer the user query.dont ask questions\n",
    "If the original user query contains BOTH a product and a content request.\n",
    "→ YOU MUST produce BOTH queries.\n",
    "\n",
    "ORIGINAL USER QUERY:\n",
    "\\\"{original_query}\\\"\n",
    "\n",
    "RULES (MANDATORY):\n",
    "- EXACTLY ONE JSON\n",
    "- NO MULTIPLE JSON\n",
    "- NEVER say \"no need to decompose\"\n",
    "- NEVER return null\n",
    "- NEVER return empty queries\n",
    "- ALWAYS generate BOTH queries\n",
    "- If a product is mentioned → it MUST appear in business_query\n",
    "- If marketing or social media is requested → it MUST appear in general_query\n",
    "- Output only JSON (no markdown, no explanation outside JSON)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{{\n",
    "  \"business_query\": \"...\",\n",
    "  \"general_query\": \"...\",\n",
    "  \"reasoning\": \"short explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # Appeler le LLM de décomposition\n",
    "    response = generate_llm_answer(decomposition_prompt)\n",
    "    print(\"*******************************************\")\n",
    "    print(response)\n",
    "    print(\"*******************************************\")\n",
    "    decomposed = extract_first_json(response)\n",
    "\n",
    "    print(\"🧠 DÉCOMPOSITION DE LA QUERY:\")\n",
    "    print(f\"  Original: {original_query}\")\n",
    "    print(f\"  Business: {decomposed['business_query']}\")\n",
    "    print(f\"  General: {decomposed['general_query']}\")\n",
    "    print(f\"  Raison: {decomposed['reasoning']}\\n\")\n",
    "\n",
    "    return decomposed\n",
    "\n",
    "# ================================\n",
    "# 🔍 RECHERCHE DANS QDRANT\n",
    "# ================================\n",
    "def search_qdrant(query, collection_name, top_k=5):\n",
    "    \"\"\"\n",
    "    Recherche dans une collection Qdrant\n",
    "    \"\"\"\n",
    "    if not query or query.strip() == \"\":\n",
    "        return []\n",
    "\n",
    "    query_vector = embedding_model.encode(query).tolist()\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    # Extraire les résultats\n",
    "    extracted = []\n",
    "    for point in results.points:\n",
    "        payload = point.payload\n",
    "        extracted.append({\n",
    "            \"id\": point.id,\n",
    "            \"score\": point.score,\n",
    "            \"text\": payload.get(\"text\", \"\"),\n",
    "            \"file_name\": payload.get(\"file_name\", \"\"),\n",
    "            \"topic\": payload.get(\"topic\", \"\"),\n",
    "            \"category\": payload.get(\"category\", \"\"),\n",
    "            \"item_id\": payload.get(\"item_id\", \"\")\n",
    "        })\n",
    "\n",
    "    return extracted\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 📝 CONSTRUCTION DU CONTEXTE\n",
    "# ================================\n",
    "def build_context(general_results, business_results, max_length=4000):\n",
    "    \"\"\"\n",
    "    Construit un contexte structuré à partir des résultats\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    current_length = 0\n",
    "\n",
    "    if business_results:\n",
    "        context_parts.append(\"=\"*80)\n",
    "        context_parts.append(\"BUSINESS INFORMATIONS\")\n",
    "        context_parts.append(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for idx, r in enumerate(business_results, 1):\n",
    "            if current_length >= max_length:\n",
    "                break\n",
    "\n",
    "            chunk = f\"[Document {idx}] Source: {r['file_name']}\\n\"\n",
    "            chunk += f\"Score de pertinence: {r['score']:.3f}\\n\"\n",
    "            chunk += f\"Contenu:\\n{r['text'][:600]}\\n\"\n",
    "            chunk += \"-\"*80 + \"\\n\"\n",
    "\n",
    "            if current_length + len(chunk) <= max_length:\n",
    "                context_parts.append(chunk)\n",
    "                current_length += len(chunk)\n",
    "\n",
    "    if general_results:\n",
    "        context_parts.append(\"\\n\" + \"=\"*80)\n",
    "        context_parts.append(\"📚 General INFORMATIONS \")\n",
    "        context_parts.append(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for idx, r in enumerate(general_results, 1):\n",
    "            if current_length >= max_length:\n",
    "                break\n",
    "\n",
    "            chunk = f\"[Document {idx}] Source: {r['file_name']}\"\n",
    "            if r['topic']:\n",
    "                chunk += f\" | Sujet: {r['topic']}\"\n",
    "            chunk += f\"\\nScore de pertinence: {r['score']:.3f}\\n\"\n",
    "            chunk += f\"Contenu:\\n{r['text'][:600]}\\n\"\n",
    "            chunk += \"-\"*80 + \"\\n\"\n",
    "\n",
    "            if current_length + len(chunk) <= max_length:\n",
    "                context_parts.append(chunk)\n",
    "                current_length += len(chunk)\n",
    "\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "# ================================\n",
    "# 🧠 LLM 2: GÉNÉRATION DE RÉPONSE\n",
    "# ================================\n",
    "def generate_final_answer(original_query, context):\n",
    "    \"\"\"\n",
    "    Generate the final answer based on the retrieved context.\n",
    "    \"\"\"\n",
    "    generation_prompt = f\"\"\"You are a creative and expert marketing assistant. Your role is to help users craft impactful marketing content and answer their marketing-related questions.\n",
    "\n",
    "    AVAILABLE CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    USER QUESTION:\n",
    "    {original_query}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Use ONLY the information provided in the context.\n",
    "    2. If the question is about a specific product, rely on the product information and customer reviews.\n",
    "    3. If the user asks for marketing content (post, email, ad, script, etc.), use the templates and best practices.\n",
    "    4. Be creative, engaging, and professional.\n",
    "    5. Structure your answer clearly.\n",
    "    6. If the context does not contain enough information, state it clearly.\n",
    "\n",
    "    ANSWER:\"\"\"\n",
    "\n",
    "    answer = generate_llm_answer(generation_prompt)\n",
    "    return answer\n",
    "# ================================\n",
    "# 🚀 PIPELINE RAG COMPLET\n",
    "# ================================\n",
    "def advanced_rag_pipeline(user_query, top_k_per_collection=5):\n",
    "    \"\"\"\n",
    "    Pipeline RAG complet avec décomposition de query\n",
    "\n",
    "    Étapes:\n",
    "    1. LLM décompose la query en business_query + general_query\n",
    "    2. Recherche dans Qdrant avec les queries optimisées\n",
    "    3. LLM génère la réponse finale avec le contexte\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"🚀 DÉMARRAGE DU PIPELINE RAG AVANCÉ\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Query originale: {user_query}\\n\")\n",
    "\n",
    "    # ÉTAPE 1: Décomposition de la query\n",
    "    print(\"📍 ÉTAPE 1: Décomposition de la query\")\n",
    "    print(\"-\"*80)\n",
    "    decomposed = decompose_query(user_query)\n",
    "\n",
    "    # ÉTAPE 2: Recherche dans les deux collections\n",
    "    print(\"\\n📍 ÉTAPE 2: Recherche dans Qdrant\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    business_results = []\n",
    "    if decomposed['business_query']:\n",
    "        print(f\"🔍 Recherche business: '{decomposed['business_query']}'\")\n",
    "        business_results = search_qdrant(\n",
    "            decomposed['business_query'],\n",
    "            \"business_docs\",\n",
    "            top_k_per_collection\n",
    "        )\n",
    "        print(f\"  ✓ Trouvé {len(business_results)} résultats business\")\n",
    "\n",
    "    general_results = []\n",
    "    if decomposed['general_query']:\n",
    "        print(f\"🔍 Recherche general: '{decomposed['general_query']}'\")\n",
    "        general_results = search_qdrant(\n",
    "            decomposed['general_query'],\n",
    "            \"general_docs\",\n",
    "            top_k_per_collection\n",
    "        )\n",
    "        print(f\"  ✓ Trouvé {len(general_results)} résultats généraux\")\n",
    "\n",
    "    # ÉTAPE 3: Construction du contexte\n",
    "    print(\"\\n📍 ÉTAPE 3: Construction du contexte\")\n",
    "    print(\"-\"*80)\n",
    "    context = build_context(general_results, business_results)\n",
    "    print(f\"  ✓ Contexte construit: {len(context)} caractères\")\n",
    "\n",
    "    # ÉTAPE 4: Génération de la réponse\n",
    "    print(\"\\n📍 ÉTAPE 4: Génération de la réponse finale\")\n",
    "    print(\"-\"*80)\n",
    "    final_answer = generate_final_answer(user_query, context)\n",
    "\n",
    "    # Résultats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ PIPELINE TERMINÉ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return {\n",
    "        \"query_original\": user_query,\n",
    "        \"query_decomposition\": decomposed,\n",
    "        \"results\": {\n",
    "            \"business\": business_results,\n",
    "            \"general\": general_results\n",
    "        },\n",
    "        \"context\": context,\n",
    "        \"answer\": final_answer\n",
    "    }\n",
    "\n",
    "# ================================\n",
    "# ⚡️ FONCTION LLM (À REMPLACER)\n",
    "# ================================\n",
    "def generate_llm_answer(prompt):\n",
    "    return llm(prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fviYsTWWSn2K",
    "outputId": "016f37c6-f2b6-4610-a3b0-ec345ba5e935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                       🧪 TEST 1: Post TikTok pour produit                       \n",
      "================================================================================\n",
      "🚀 DÉMARRAGE DU PIPELINE RAG AVANCÉ\n",
      "================================================================================\n",
      "Query originale: how i acheive my objectives with using SEO\n",
      "\n",
      "📍 ÉTAPE 1: Décomposition de la query\n",
      "--------------------------------------------------------------------------------\n",
      "*******************************************\n",
      "\n",
      "You are a QUERY ROUTER for a multi-database RAG system.first you should understand the intent of the query and what we need from general data and what we need from buisness data to answer the user query .\n",
      "and you should answer with one json answer.\n",
      "DATABASES:\n",
      "\n",
      "GENERAL_DOCS contains:\n",
      "- Marketing templates,Social media strategies,TikTok, Instagram, YouTube content informations,Brand positioning theory\n",
      "\n",
      "BUSINESS_DOCS contains:\n",
      "- Products (name, description, materials, price, category, supplier) , Brand tone and values ,the objectives,KPIs ,Customer profiles,Customer questions and Customerreviews\n",
      "\n",
      "YOUR TASK:\n",
      "You MUST always analyse and decompose the query into two search queries:\n",
      "1) ONE query for GENERAL_DOCS (marketing knowledge, templates, strategy)->a detailed query ask question what best techniques what best strategies what the methodes to answer the user query\n",
      "2) ONE query for BUSINESS_DOCS (product, brand, customer, KPIs)->extract from business data  the elements to answer the user query.dont ask questions\n",
      "If the original user query contains BOTH a product and a content request.\n",
      "→ YOU MUST produce BOTH queries.\n",
      "\n",
      "ORIGINAL USER QUERY:\n",
      "\"how i acheive my objectives with using SEO\"\n",
      "\n",
      "RULES (MANDATORY):\n",
      "- EXACTLY ONE JSON\n",
      "- NO MULTIPLE JSON\n",
      "- NEVER say \"no need to decompose\"\n",
      "- NEVER return null\n",
      "- NEVER return empty queries\n",
      "- ALWAYS generate BOTH queries\n",
      "- If a product is mentioned → it MUST appear in business_query\n",
      "- If marketing or social media is requested → it MUST appear in general_query\n",
      "- Output only JSON (no markdown, no explanation outside JSON)\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "{\n",
      "  \"business_query\": \"...\",\n",
      "  \"general_query\": \"...\",\n",
      "  \"reasoning\": \"short explanation\"\n",
      "}\n",
      "```json\n",
      "{\n",
      "  \"business_query\": \"What are the key performance indicators (KPIs) related to achieving specific business goals?\",\n",
      "  \"general_query\": \"How can I effectively use Search Engine Optimization (SEO) to achieve my objectives?\",\n",
      "  \"reasoning\": \"The original query mentions both SEO and products. The 'business_query' will address the KPIs that align with business goals, while the 'general_query' will provide methods for implementing SEO.\"\n",
      "}\n",
      "```\n",
      "*******************************************\n",
      "🧠 DÉCOMPOSITION DE LA QUERY:\n",
      "  Original: how i acheive my objectives with using SEO\n",
      "  Business: What are the key performance indicators (KPIs) related to achieving specific business goals?\n",
      "  General: How can I effectively use Search Engine Optimization (SEO) to achieve my objectives?\n",
      "  Raison: The original query mentions both SEO and products. The 'business_query' will address the KPIs that align with business goals, while the 'general_query' will provide methods for implementing SEO.\n",
      "\n",
      "\n",
      "📍 ÉTAPE 2: Recherche dans Qdrant\n",
      "--------------------------------------------------------------------------------\n",
      "🔍 Recherche business: 'What are the key performance indicators (KPIs) related to achieving specific business goals?'\n",
      "  ✓ Trouvé 5 résultats business\n",
      "🔍 Recherche general: 'How can I effectively use Search Engine Optimization (SEO) to achieve my objectives?'\n",
      "  ✓ Trouvé 5 résultats généraux\n",
      "\n",
      "📍 ÉTAPE 3: Construction du contexte\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Contexte construit: 4317 caractères\n",
      "\n",
      "📍 ÉTAPE 4: Génération de la réponse finale\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "✅ PIPELINE TERMINÉ\n",
      "================================================================================\n",
      "\n",
      "📝 RÉPONSE FINALE:\n",
      "You are a creative and expert marketing assistant. Your role is to help users craft impactful marketing content and answer their marketing-related questions.\n",
      "\n",
      "    AVAILABLE CONTEXT:\n",
      "    ================================================================================\n",
      "BUSINESS INFORMATIONS\n",
      "================================================================================\n",
      "\n",
      "[Document 1] Source: kpis.json\n",
      "Score de pertinence: 0.395\n",
      "Contenu:\n",
      "{\"kpi_name\": \"Customer Satisfaction\", \"description\": \"Maintain exceptional service and product quality.\", \"target\": \"4.5/5 average rating\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 2] Source: kpis.json\n",
      "Score de pertinence: 0.362\n",
      "Contenu:\n",
      "{\"kpi_name\": \"Website Traffic\", \"description\": \"Boost traffic with a premium, smooth, and inspiring digital experience.\", \"target\": \"20% increase per month\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 3] Source: kpis.json\n",
      "Score de pertinence: 0.343\n",
      "Contenu:\n",
      "{\"kpi_name\": \"Social Media Engagement\", \"description\": \"Increase interactions through elegant, confidence-driven content.\", \"target\": \"10% monthly growth\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 4] Source: kpis.json\n",
      "Score de pertinence: 0.332\n",
      "Contenu:\n",
      "{\"kpi_name\": \"Conversion Rate\", \"description\": \"Convert visitors through elevated product presentation and seamless UX.\", \"target\": \"5% purchase conversion\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 5] Source: kpis.json\n",
      "Score de pertinence: 0.320\n",
      "Contenu:\n",
      "{\"kpi_name\": \"Sell-Through Rate\", \"description\": \"Ensure rapid collection adoption while maintaining exclusivity.\", \"target\": \"80% sell-through in 3 months\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📚 General INFORMATIONS \n",
      "================================================================================\n",
      "\n",
      "[Document 1] Source: Blogging-Marketing-Course-eMarketing-Institute-Ebook-2018-Edition.pdf\n",
      "Score de pertinence: 0.611\n",
      "Contenu:\n",
      " basics of SEO. Search engine optimization is the process of optimizing\n",
      "\n",
      "and improving the website (or the blog) to increase its rank in the search engine result pages\n",
      "\n",
      "for the relevant search queries. So you have a specific keyword or several keywords that are\n",
      "\n",
      "relevant for your business. Online users who look for these keywords are likely to be interested\n",
      "\n",
      "in your business. Your goal with SEO is to improve your online presence, so these users can find\n",
      "\n",
      "your website when they search relevant keywords.\n",
      "\n",
      "Regarding SEO, when you appear in the search engine result pages is very important. It is s\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 2] Source: Blogging-Marketing-Course-eMarketing-Institute-Ebook-2018-Edition.pdf\n",
      "Score de pertinence: 0.609\n",
      "Contenu:\n",
      "and other relevant data that can help you organize future campaigns more \n",
      "efficiently.   \n",
      "Blogging and SEO  \n",
      "To understand the relation ship between blogging and SEO  (search engine optimization ), you \n",
      "have to understand the basics of SEO. Search engine optimization is the process of optimizing \n",
      "and improving the website (or the blog)  to increase its rank in th e search engine result pages \n",
      "for the relevant search queries. So you have a specific keyword or several keyword s that are \n",
      "relevant for your business. Online users who look for these keywords are likely to be interested \n",
      "in your bus\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 3] Source: Blogging-Marketing-Course-eMarketing-Institute-Ebook-2018-Edition.pdf\n",
      "Score de pertinence: 0.582\n",
      "Contenu:\n",
      " the search engine result pages.\n",
      "\n",
      "Even Bill Gates predicted the importance of content back in 1996, but when it comes to modern\n",
      "online marketing and strategies that are available to you, here is why content marketing really\n",
      "matters.\n",
      "\n",
      "Content marketing and SEO\n",
      "\n",
      "If your business is oriented towards online promotion and connection with online users, you're\n",
      "already familiar with the concept of SEO, as the process of optimizing your website in the purpose\n",
      "of being positioned high up in the search engine result pages. The concept of search engine\n",
      "optimization has been evolving as the amount of conte\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    USER QUESTION:\n",
      "    how i acheive my objectives with using SEO\n",
      "\n",
      "    INSTRUCTIONS:\n",
      "    1. Use ONLY the information provided in the context.\n",
      "    2. If the question is about a specific product, rely on the product information and customer reviews.\n",
      "    3. If the user asks for marketing content (post, email, ad, script, etc.), use the templates and best practices.\n",
      "    4. Be creative, engaging, and professional.\n",
      "    5. Structure your answer clearly.\n",
      "    6. If the context does not contain enough information, state it clearly.\n",
      "\n",
      "    ANSWER: \n",
      "    To achieve your objectives with using SEO, first identify the target audience and the keywords that are most relevant to them. This will allow you to optimize your website's content for those specific terms. Next, focus on creating high-quality, informative, and engaging content that provides value to your readers. Regularly update this content to keep it fresh and appealing. Additionally, ensure that your website is mobile-friendly and easy to navigate, as many users now access websites from their smartphones. Utilize social media platforms to share your content and engage with potential customers. Finally, monitor your website's performance regularly by tracking metrics such as bounce rate, time on page, and conversions. Adjust your strategy based on what works best for your audience and the competition. By focusing on these key areas, you'll be well on your way to achieving your SEO goals and increasing visibility on search engines. 🚀💡 #SEO #OnlineMarketing #SearchEngineOptimization\n",
      "    ---------------------------------------\n",
      "    Q: How do I create an effective landing page for my products?\n",
      "\n",
      "    A: Creating an effective landing page is crucial for converting visitors into customers. Here’s how you can design one:\n",
      "\n",
      "    1. **Define Your Purpose**: Clearly define what you want the landing page to accomplish. Is it to showcase\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Test 1: Query sur un produit spécifique\n",
    "    print(\"\\n\" + \"🧪 TEST 1: Post TikTok pour produit\".center(80))\n",
    "    result1 = advanced_rag_pipeline(\n",
    "        \"how i acheive my objectives with using SEO\"\n",
    "    )\n",
    "    print(\"\\n📝 RÉPONSE FINALE:\")\n",
    "    print(result1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ftGm6NvCw1u",
    "outputId": "b4e42772-43d6-4af3-ae83-ed63d577ed45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                       🧪 TEST 1: Post TikTok pour produit                       \n",
      "================================================================================\n",
      "🚀 DÉMARRAGE DU PIPELINE RAG AVANCÉ\n",
      "================================================================================\n",
      "Query originale: write an ads for Luna Denim Jacket\n",
      "\n",
      "📍 ÉTAPE 1: Décomposition de la query\n",
      "--------------------------------------------------------------------------------\n",
      "*******************************************\n",
      "\n",
      "You are a QUERY ROUTER for a multi-database RAG system.first you should understand the intent of the query and what we need from general data and what we need from buisness data to answer the user query .\n",
      "and you should answer with one json answer.\n",
      "DATABASES:\n",
      "\n",
      "GENERAL_DOCS contains:\n",
      "- Marketing templates,Social media strategies,TikTok, Instagram, YouTube content informations,Brand positioning theory\n",
      "\n",
      "BUSINESS_DOCS contains:\n",
      "- Products (name, description, materials, price, category, supplier) , Brand tone and values ,the objectives,KPIs ,Customer profiles,Customer questions and Customerreviews\n",
      "\n",
      "YOUR TASK:\n",
      "You MUST always analyse and decompose the query into two search queries:\n",
      "1) ONE query for GENERAL_DOCS (marketing knowledge, templates, strategy)->a detailed query ask question what best techniques what best strategies what the methodes to answer the user query\n",
      "2) ONE query for BUSINESS_DOCS (product, brand, customer, KPIs)->extract from business data  the elements to answer the user query.dont ask questions\n",
      "If the original user query contains BOTH a product and a content request.\n",
      "→ YOU MUST produce BOTH queries.\n",
      "\n",
      "ORIGINAL USER QUERY:\n",
      "\"write an ads for Luna Denim Jacket\"\n",
      "\n",
      "RULES (MANDATORY):\n",
      "- EXACTLY ONE JSON\n",
      "- NO MULTIPLE JSON\n",
      "- NEVER say \"no need to decompose\"\n",
      "- NEVER return null\n",
      "- NEVER return empty queries\n",
      "- ALWAYS generate BOTH queries\n",
      "- If a product is mentioned → it MUST appear in business_query\n",
      "- If marketing or social media is requested → it MUST appear in general_query\n",
      "- Output only JSON (no markdown, no explanation outside JSON)\n",
      "\n",
      "OUTPUT FORMAT:\n",
      "{\n",
      "  \"business_query\": \"...\",\n",
      "  \"general_query\": \"...\",\n",
      "  \"reasoning\": \"short explanation\"\n",
      "}\n",
      "```json\n",
      "{\n",
      "  \"business_query\": \"Write product details about Luna Denim Jacket including name, description, materials, price, category, and supplier.\",\n",
      "  \"general_query\": \"Develop TikTok, Instagram, and YouTube content ideas for promoting the Luna Denim Jacket using current trends and audience interests.\",\n",
      "  \"reasoning\": \"The user requests both a content promotion plan and product information. The business_query fetches product details while the general_query generates relevant content ideas.\"\n",
      "}\n",
      "```\n",
      "*******************************************\n",
      "🧠 DÉCOMPOSITION DE LA QUERY:\n",
      "  Original: write an ads for Luna Denim Jacket\n",
      "  Business: Write product details about Luna Denim Jacket including name, description, materials, price, category, and supplier.\n",
      "  General: Develop TikTok, Instagram, and YouTube content ideas for promoting the Luna Denim Jacket using current trends and audience interests.\n",
      "  Raison: The user requests both a content promotion plan and product information. The business_query fetches product details while the general_query generates relevant content ideas.\n",
      "\n",
      "\n",
      "📍 ÉTAPE 2: Recherche dans Qdrant\n",
      "--------------------------------------------------------------------------------\n",
      "🔍 Recherche business: 'Write product details about Luna Denim Jacket including name, description, materials, price, category, and supplier.'\n",
      "  ✓ Trouvé 5 résultats business\n",
      "🔍 Recherche general: 'Develop TikTok, Instagram, and YouTube content ideas for promoting the Luna Denim Jacket using current trends and audience interests.'\n",
      "  ✓ Trouvé 5 résultats généraux\n",
      "\n",
      "📍 ÉTAPE 3: Construction du contexte\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Contexte construit: 3764 caractères\n",
      "\n",
      "📍 ÉTAPE 4: Génération de la réponse finale\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "✅ PIPELINE TERMINÉ\n",
      "================================================================================\n",
      "\n",
      "📝 RÉPONSE FINALE:\n",
      "You are a creative and expert marketing assistant. Your role is to help users craft impactful marketing content and answer their marketing-related questions.\n",
      "\n",
      "    AVAILABLE CONTEXT:\n",
      "    ================================================================================\n",
      "BUSINESS INFORMATIONS\n",
      "================================================================================\n",
      "\n",
      "[Document 1] Source: Customer Reviews.json\n",
      "Score de pertinence: 0.750\n",
      "Contenu:\n",
      "{\"product_name\": \"Luna Denim Jacket\", \"review\": \"The denim jacket is stylish and durable. I wear it over dresses and jeans, perfect fit.\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 2] Source: Products.json\n",
      "Score de pertinence: 0.740\n",
      "Contenu:\n",
      "{\"type\": \"product\", \"product_name\": \"Luna Denim Jacket\", \"description\": \"Premium slim-fit denim jacket with durable stitching, ideal for layering over dresses, shirts, or casual wear. A versatile wardrobe staple suitable for urban streetwear and casual outings.\", \"sizes\": [\"S\", \"M\", \"L\", \"XL\"], \"colors\": [\"Indigo Blue\", \"Black\", \"Light Wash\"], \"supplier\": \"DenimPro Ltd.\", \"materials\": \"98% cotton, 2% elastane\", \"factory\": \"Urban Stitch Apparel, Tunisia\", \"category\": \"jacket\", \"language\": \"en\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 3] Source: Customer Reviews.json\n",
      "Score de pertinence: 0.711\n",
      "Contenu:\n",
      "{\"product_name\": \"Luna Denim Jacket\", \"review\": \"Color Indigo Blue looks exactly like the picture. Stitching is sturdy, good quality.\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 4] Source: Customer Reviews.json\n",
      "Score de pertinence: 0.697\n",
      "Contenu:\n",
      "{\"product_name\": \"Luna Denim Jacket\", \"review\": \"Black color is deep and stylish. Material is thicker than expected, very nice.\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 5] Source: Customer Reviews.json\n",
      "Score de pertinence: 0.690\n",
      "Contenu:\n",
      "{\"product_name\": \"Luna Denim Jacket\", \"review\": \"Versatile piece, goes with casual and semi-formal outfits. Received many compliments.\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📚 General INFORMATIONS \n",
      "================================================================================\n",
      "\n",
      "[Document 1] Source: TikTok Marketing.pdf | Sujet: tiktok\n",
      "Score de pertinence: 0.596\n",
      "Contenu:\n",
      " #party \n",
      "85. #instacool \n",
      "86. #christmas \n",
      "87. #fit \n",
      "88. #goodmorning \n",
      "89. #workout \n",
      "90. #blue \n",
      "91. #flowers 92. #handmade \n",
      "93. #blackandwhite \n",
      "94. #instafood \n",
      "95. #yummy \n",
      "96. #pink \n",
      "97. #hot \n",
      "98. #lifestyle \n",
      "99. #work \n",
      "100. #black \n",
      "Take a good look at these hashtag suggestions and work out which ones will suit your niche. Some of \n",
      "them are general so you can experiment with these. Always measure your hashtag performance as it \n",
      "is very important for your success on TikTok. We believe that there are three effective ways that you can market your brand on TikTok: \n",
      "1. You create your own branded cha\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Document 2] Source: TikTok Marketing.pdf | Sujet: tiktok\n",
      "Score de pertinence: 0.595\n",
      "Contenu:\n",
      " up \n",
      "getting around to it. \n",
      "So if you want to create a challenge put a time limit on it. When a TikTok user knows that they only \n",
      "have a few days to participate it will force them to make a decision. GUESS Jeans gave users 7 days to \n",
      "participate in their challenge which was very effective as their participation numbers were high. \n",
      "5. Be Authentic \n",
      " \n",
      "We would always recommend authenticity with your marketing for any social channel but it is \n",
      "particularly important with TikTok as it is so new and pure. The platform is not saturated with brands \n",
      "yet like Instagram and others are (we are sure that\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    USER QUESTION:\n",
      "    write an ads for Luna Denim Jacket\n",
      "\n",
      "    INSTRUCTIONS:\n",
      "    1. Use ONLY the information provided in the context.\n",
      "    2. If the question is about a specific product, rely on the product information and customer reviews.\n",
      "    3. If the user asks for marketing content (post, email, ad, script, etc.), use the templates and best practices.\n",
      "    4. Be creative, engaging, and professional.\n",
      "    5. Structure your answer clearly.\n",
      "    6. If the context does not contain enough information, state it clearly.\n",
      "\n",
      "    ANSWER: \n",
      "\n",
      "    ### Ad Campaign: Elevate Your Style with Luna Denim Jacket!\n",
      "\n",
      "    🌟 **Introducing Luna Denim Jacket** – Your Ultimate Stylish Partner! 💕\n",
      "\n",
      "    Are you tired of looking plain? Our Luna Denim Jacket has got you covered! Designed with durability and versatility in mind, this premium slim-fit denim jacket is the perfect addition to your wardrobe.\n",
      "\n",
      "    🛍️ **Why Choose Luna Denim Jacket?**\n",
      "    \n",
      "    - **Durable Durability:** Say goodbye to fraying edges; our jacket features sturdy stitching, ensuring longevity and comfort.\n",
      "    - **Versatile Fit:** Wear it over dresses, shirts, or jeans—perfect for layering and adding style to your everyday outfit.\n",
      "    - **Color Variety:** Choose from stunning colors like Indigo Blue, Black, and Light Wash, each one crafted with care to stand out.\n",
      "    - **Material Quality:** Crafted from premium materials, making it both comfortable and stylish.\n",
      "    - **Fit Guarantee:** Enjoy a perfectly tailored fit with sizes ranging from S to XL, ensuring everyone finds their perfect size.\n",
      "    - **Factory Made in Urban Stitch Apparel, Tunisia**: Quality assurance backed by local craftsmanship.\n",
      "    \n",
      "    ⚡️ **How To Order Now?**\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"🧪 TEST 1: Post TikTok pour produit\".center(80))\n",
    "    result1 = advanced_rag_pipeline(\n",
    "        \"write an ads for Luna Denim Jacket\"\n",
    "    )\n",
    "    print(\"\\n📝 RÉPONSE FINALE:\")\n",
    "    print(result1['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz41P42mD6XK"
   },
   "source": [
    "# **CHATBOT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V92wseIMEN0G"
   },
   "source": [
    "**llm model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPl2HHkdERI9",
    "outputId": "3d8e0fd4-95ba-485c-b00e-2734adc6a363"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-I-BJt_96Od"
   },
   "source": [
    "**reitreive context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YF-3KyzAu5w2"
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_first_json(response_text):\n",
    "    \"\"\"\n",
    "    Extrait le premier objet JSON trouvé dans une réponse texte.\n",
    "    \"\"\"\n",
    "    json_block_match = re.search(r\"```json\\s*(.*?)\\s*```\", response_text, re.DOTALL)\n",
    "    if json_block_match:\n",
    "        json_string = json_block_match.group(1)\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"⚠️ Erreur JSON (bloc markdown):\", e)\n",
    "            print(\"Texte JSON brut:\", json_string)\n",
    "    match = re.search(r\"\\{.*?\\}\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"⚠️ Erreur JSON (general search):\", e)\n",
    "            print(\"Texte JSON brut:\", match.group())\n",
    "            return None\n",
    "    else:\n",
    "        print(\"⚠️ Aucun JSON trouvé dans la réponse.\")\n",
    "        return None\n",
    "# ================================\n",
    "# 🧠 LLM 1: QUERY DECOMPOSITION\n",
    "# ================================\n",
    "def decompose_query(original_query):\n",
    "\n",
    "    decomposition_prompt = f\"\"\"\n",
    "You are a QUERY ROUTER for a multi-database RAG system.first you should understand the intent of the query and what we need from general data and what we need from buisness data to answer the user query .\n",
    "and you should answer with one json answer.\n",
    "DATABASES:\n",
    "\n",
    "GENERAL_DOCS contains:\n",
    "- Marketing templates,Social media strategies,TikTok, Instagram, YouTube content informations,Brand positioning theory\n",
    "\n",
    "BUSINESS_DOCS contains:\n",
    "- Products (name, description, materials, price, category, supplier) , Brand tone and values ,the objectives,KPIs ,Customer profiles,Customer questions and Customerreviews\n",
    "\n",
    "YOUR TASK:\n",
    "You MUST always analyse and decompose the query into two search queries:\n",
    "1) ONE query for GENERAL_DOCS (marketing knowledge, templates, strategy)->a detailed query ask question what best techniques what best strategies what the methodes to answer the user query\n",
    "2) ONE query for BUSINESS_DOCS (product, brand, customer, KPIs)->extract from business data  the elements to answer the user query.dont ask questions\n",
    "If the original user query contains BOTH a product and a content request.\n",
    "→ YOU MUST produce BOTH queries.\n",
    "\n",
    "ORIGINAL USER QUERY:\n",
    "\\\"{original_query}\\\"\n",
    "\n",
    "RULES (MANDATORY):\n",
    "- EXACTLY ONE JSON\n",
    "- NO MULTIPLE JSON\n",
    "- NEVER say \"no need to decompose\"\n",
    "- NEVER return null\n",
    "- NEVER return empty queries\n",
    "- ALWAYS generate BOTH queries\n",
    "- If a product is mentioned → it MUST appear in business_query\n",
    "- If marketing or social media is requested → it MUST appear in general_query\n",
    "- Output only JSON (no markdown, no explanation outside JSON)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{{\n",
    "  \"business_query\": \"...\",\n",
    "  \"general_query\": \"...\",\n",
    "  \"reasoning\": \"short explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # Appeler le LLM de décomposition\n",
    "    response = generate_llm_answer(decomposition_prompt)\n",
    "    decomposed = extract_first_json(response)\n",
    "\n",
    "    print(\"🧠 DÉCOMPOSITION DE LA QUERY:\")\n",
    "    print(f\"  Original: {original_query}\")\n",
    "    print(f\"  Business: {decomposed['business_query']}\")\n",
    "    print(f\"  General: {decomposed['general_query']}\")\n",
    "    print(f\"  Raison: {decomposed['reasoning']}\\n\")\n",
    "\n",
    "    return decomposed\n",
    "\n",
    "# ================================\n",
    "# 🔍 RECHERCHE DANS QDRANT\n",
    "# ================================\n",
    "def search_qdrant(query, collection_name, top_k=7):\n",
    "    \"\"\"\n",
    "    Recherche dans une collection Qdrant\n",
    "    \"\"\"\n",
    "    if not query or query.strip() == \"\":\n",
    "        return []\n",
    "\n",
    "    query_vector = embedding_model.encode(query).tolist()\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    extracted = []\n",
    "    for point in results.points:\n",
    "        payload = point.payload\n",
    "        extracted.append({\n",
    "            \"id\": point.id,\n",
    "            \"score\": point.score,\n",
    "            \"text\": payload.get(\"text\", \"\"),\n",
    "            \"file_name\": payload.get(\"file_name\", \"\"),\n",
    "            \"topic\": payload.get(\"topic\", \"\"),\n",
    "            \"category\": payload.get(\"category\", \"\"),\n",
    "            \"item_id\": payload.get(\"item_id\", \"\")\n",
    "        })\n",
    "\n",
    "    return extracted\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 📝 CONSTRUCTION DU CONTEXTE\n",
    "# ================================\n",
    "def build_context(general_results, business_results, max_length=4000):\n",
    "    \"\"\"\n",
    "    Construit un contexte structuré à partir des résultats\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    current_length = 0\n",
    "\n",
    "    if business_results:\n",
    "        context_parts.append(\"=\"*80)\n",
    "        context_parts.append(\"BUSINESS INFORMATIONS\")\n",
    "        context_parts.append(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for idx, r in enumerate(business_results, 1):\n",
    "            if current_length >= max_length:\n",
    "                break\n",
    "\n",
    "            chunk = f\"[Document {idx}] Source: {r['file_name']}\\n\"\n",
    "            chunk += f\"Score de pertinence: {r['score']:.3f}\\n\"\n",
    "            chunk += f\"Contenu:\\n{r['text'][:600]}\\n\"\n",
    "            chunk += \"-\"*80 + \"\\n\"\n",
    "\n",
    "            if current_length + len(chunk) <= max_length:\n",
    "                context_parts.append(chunk)\n",
    "                current_length += len(chunk)\n",
    "\n",
    "    if general_results:\n",
    "        context_parts.append(\"\\n\" + \"=\"*80)\n",
    "        context_parts.append(\"📚 General INFORMATIONS \")\n",
    "        context_parts.append(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for idx, r in enumerate(general_results, 1):\n",
    "            if current_length >= max_length:\n",
    "                break\n",
    "\n",
    "            chunk = f\"[Document {idx}] Source: {r['file_name']}\"\n",
    "            if r['topic']:\n",
    "                chunk += f\" | Sujet: {r['topic']}\"\n",
    "            chunk += f\"\\nScore de pertinence: {r['score']:.3f}\\n\"\n",
    "            chunk += f\"Contenu:\\n{r['text'][:600]}\\n\"\n",
    "            chunk += \"-\"*80 + \"\\n\"\n",
    "\n",
    "            if current_length + len(chunk) <= max_length:\n",
    "                context_parts.append(chunk)\n",
    "                current_length += len(chunk)\n",
    "\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "# ================================\n",
    "# 🧠 LLM 2: GÉNÉRATION DE RÉPONSE\n",
    "# ================================\n",
    "def generate_final_answer(original_query, context):\n",
    "    \"\"\"\n",
    "    Generate the final answer based on the retrieved context.\n",
    "    \"\"\"\n",
    "    generation_prompt =f\"\"\"You are a creative and expert marketing assistant. Your role is to help users craft impactful marketing content and answer their marketing-related questions.\n",
    "\n",
    "    AVAILABLE CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    USER QUESTION:\n",
    "    {original_query}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Use ONLY the information provided in the context.\n",
    "    2. If the question is about a specific product, rely on the product information and customer reviews.\n",
    "    3. If the user asks for marketing content (post, email, ad, script, etc.), use the templates and best practices.\n",
    "    4. Be creative, engaging, and professional.\n",
    "    5. Structure your answer clearly.\n",
    "    6. If the context does not contain enough information, state it clearly.\n",
    "\n",
    "    ANSWER:\"\"\"\n",
    "\n",
    "    answer = generate_llm_answer(generation_prompt)\n",
    "    return answer\n",
    "# ================================\n",
    "# 🚀 PIPELINE RAG COMPLET\n",
    "# ================================\n",
    "def retrieve_context(user_query,top_k_per_collection=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant information about marketing and specific buisness   for the LLM to use automatically.\n",
    "    \"\"\"\n",
    "    decomposed = decompose_query(user_query)\n",
    "    business_results = []\n",
    "    if decomposed['business_query']:\n",
    "        print(f\"🔍 Recherche business: '{decomposed['business_query']}'\")\n",
    "        business_results = search_qdrant(\n",
    "            decomposed['business_query'],\n",
    "            \"business_docs\",\n",
    "            top_k_per_collection\n",
    "        )\n",
    "        print(f\"  ✓ Trouvé {len(business_results)} résultats business\")\n",
    "\n",
    "    general_results = []\n",
    "    if decomposed['general_query']:\n",
    "        print(f\"🔍 Recherche general: '{decomposed['general_query']}'\")\n",
    "        general_results = search_qdrant(\n",
    "            decomposed['general_query'],\n",
    "            \"general_docs\",\n",
    "            top_k_per_collection\n",
    "        )\n",
    "    context = build_context(general_results, business_results)\n",
    "\n",
    "    return  context\n",
    "# ================================\n",
    "# ⚡️ FONCTION LLM (À REMPLACER)\n",
    "# ================================\n",
    "def generate_llm_answer(prompt):\n",
    "    return llm(prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jk8kRrOW7DlV",
    "outputId": "d21bde23-a4d0-472b-8427-08877efb9e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Long-term memory manager created!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# IMPORTS\n",
    "# =========================================\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_memory_manager\n",
    "from pydantic import BaseModel, Field\n",
    "import gradio as gr\n",
    "\n",
    "# =========================================\n",
    "# DEFINE USER / BUSINESS PROFILE SCHEMA\n",
    "# =========================================\n",
    "class UserBusinessProfile(BaseModel):\n",
    "    \"\"\"Automatically store all relevant user and business info.\"\"\"\n",
    "    user_name: str | None = Field(None, description=\"User's full name\")\n",
    "    preferred_name: str | None = Field(None, description=\"Name user prefers to be called\")\n",
    "    response_style_preference: str | None = Field(None, description=\"Preferred communication style\")\n",
    "    special_skills: list[str] = Field(default_factory=list, description=\"User's skills and expertise\")\n",
    "    business_name: str | None = Field(None, description=\"Name of user's business\")\n",
    "    business_events: list[str] = Field(default_factory=list, description=\"Important business events\")\n",
    "    partnerships: list[str] = Field(default_factory=list, description=\"Business partnerships\")\n",
    "    history: list[str] = Field(default_factory=list, description=\"Business history and milestones\")\n",
    "    other_preferences: list[str] = Field(default_factory=list, description=\"Other preferences and notes\")\n",
    "\n",
    "# ================================\n",
    "# 🔑 Groq LLM (FREE & FAST)\n",
    "# ================================\n",
    "groq_api_key = \"API KEY\"\n",
    "chat_model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    temperature=0.3,\n",
    "    max_tokens=512\n",
    ")\n",
    "# =========================================\n",
    "# CREATE LONG-TERM MEMORY MANAGER\n",
    "# =========================================\n",
    "long_term_manager = create_memory_manager(\n",
    "    chat_model,\n",
    "    schemas=[UserBusinessProfile],\n",
    "    instructions=\"Extract all noteworthy facts, events, business relationships, history, and user preferences from the conversation.\",\n",
    "    enable_inserts=True\n",
    ")\n",
    "\n",
    "print(\"✅ Long-term memory manager created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBqj9cb-7Lwe",
    "outputId": "c5efe296-4ffc-4800-9311-e0d3b24c227d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent with memory tools created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2747132183.py:53: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# 🧠 Memory Functions\n",
    "# ================================\n",
    "conversation_memory = {}\n",
    "\n",
    "def update_memory(user_id: str, messages: list):\n",
    "    \"\"\"Update long-term memory with new information from the conversation.\"\"\"\n",
    "    try:\n",
    "        result = long_term_manager.invoke(\n",
    "            {\"messages\": messages},\n",
    "            config={\"configurable\": {\"user_id\": user_id}}\n",
    "        )\n",
    "        print(f\"✅ Memory updated for user: {user_id}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error updating memory: {e}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_memory(user_id: str) -> str:\n",
    "    \"\"\"Retrieve stored memories for a user.\"\"\"\n",
    "    try:\n",
    "        if user_id in conversation_memory and conversation_memory[user_id]:\n",
    "            context_parts = []\n",
    "            for turn in conversation_memory[user_id]:\n",
    "                user_msg = turn['user']\n",
    "                if any(keyword in user_msg.lower() for keyword in ['my name is', 'i am', 'i\\'m']):\n",
    "                    context_parts.append(f\"User introduction: {user_msg}\")\n",
    "                elif any(keyword in user_msg.lower() for keyword in ['business', 'company', 'shop', 'store']):\n",
    "                    context_parts.append(f\"Business info: {user_msg}\")\n",
    "                elif any(keyword in user_msg.lower() for keyword in ['partner', 'collaboration']):\n",
    "                    context_parts.append(f\"Partnership: {user_msg}\")\n",
    "                elif any(keyword in user_msg.lower() for keyword in ['prefer', 'like', 'style']):\n",
    "                    context_parts.append(f\"Preference: {user_msg}\")\n",
    "\n",
    "            if context_parts:\n",
    "                return \"\\n\".join(context_parts)\n",
    "\n",
    "        return \"No previous context found for this user.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error retrieving memory: {e}\")\n",
    "        return \"No previous context found.\"\n",
    "\n",
    "# ================================\n",
    "# 🤖 CREATE AGENT WITH TOOLS\n",
    "# ================================\n",
    "agent = create_react_agent(\n",
    "    chat_model,\n",
    "    tools=[retrieve_context],\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=InMemoryStore()\n",
    ")\n",
    "\n",
    "print(\"✅ Agent with memory tools created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31m2dyAV7Oaj"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 📝 Build System Prompt with Memory\n",
    "# ================================\n",
    "def build_system_prompt(user_id: str) -> str:\n",
    "    \"\"\"Build system prompt with user's memory context.\"\"\"\n",
    "    memory_context = retrieve_memory(user_id)\n",
    "\n",
    "    return f\"\"\"You are a creative , personalized  and an expert marketing assistant for business owners. Your role is to help users craft impactful marketing content and answer their marketing-related questions.\n",
    "\n",
    "Use the user's profile and the tools at your disposal to answer questions.\n",
    "If the user asks something about  his buisness and something specific ALWAYS call the 'retrieve_context' tool to search the provided documents.\n",
    "Use retrieved information in your answer naturally.\n",
    "**USER CONTEXT:**\n",
    "{memory_context}\n",
    "\n",
    "**YOUR ROLE:**\n",
    "- Use the context above to personalize all responses\n",
    "- Remember user preferences, business details, and history\n",
    "- Adapt your communication style based on user preferences\n",
    "- Be helpful, professional, and proactive\n",
    "- Provide marketing insights and strategies tailored to the user's business\n",
    "- You can use the retrieve_context tool to get updated information about the user\n",
    "Rules:\n",
    "- Adapt answers to the user's business context.\n",
    "- Only call tools when necessary.\n",
    "- Keep answers natural and clear.\n",
    "- dont metion the sources or what you do\n",
    "- always personalized the answer for the buisness of the user\n",
    "\n",
    "If you don't have enough context, ask clarifying questions to learn more about the user's business.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4LHqU0P7T7x"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 💬 Chat Function with Agent\n",
    "# ================================\n",
    "def chat_function(message, history):\n",
    "    \"\"\"Main chat function that uses the agent with memory.\"\"\"\n",
    "    user_id = \"user1\"\n",
    "    # Initialize conversation memory for this user\n",
    "    if user_id not in conversation_memory:\n",
    "        conversation_memory[user_id] = []\n",
    "\n",
    "    # 1. Store user message\n",
    "    conversation_memory[user_id].append({\n",
    "        \"user\": message,\n",
    "        \"assistant\": \"\"\n",
    "    })\n",
    "\n",
    "    # 2. Build system prompt with memory context\n",
    "    system_prompt = build_system_prompt(user_id)\n",
    "\n",
    "    # 3. Prepare messages for the agent\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "\n",
    "    # 4. Invoke agent\n",
    "    try:\n",
    "        response = agent.invoke(\n",
    "            {\"messages\": messages},\n",
    "            config={\n",
    "                \"configurable\": {\n",
    "                    \"thread_id\": \"1\",\n",
    "                    \"user_id\": user_id\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        if isinstance(response, dict) and \"messages\" in response:\n",
    "            answer = response[\"messages\"][-1].content\n",
    "        else:\n",
    "            answer = str(response)\n",
    "\n",
    "        # 5. Store assistant response\n",
    "        conversation_memory[user_id][-1][\"assistant\"] = answer\n",
    "\n",
    "        # 6. Update long-term memory\n",
    "        messages_to_store = [\n",
    "            HumanMessage(content=message),\n",
    "            AIMessage(content=answer)\n",
    "        ]\n",
    "        update_memory(user_id, messages_to_store)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"I apologize, but I encountered an error. Please try again.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-R4pDH8LjGO4"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ✅ Gradio UI\n",
    "# ================================\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"# 🤖 Dynamic Personalized Marketing Assistant\n",
    "    ### The assistant learns your business dynamically using AI agent with memory tools\"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    msg = gr.Textbox(\n",
    "        label=\"Your Message\",\n",
    "        placeholder=\"Tell me about your business...\",\n",
    "        lines=2\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Send 🚀\", variant=\"primary\")\n",
    "\n",
    "    memory_display = gr.Textbox(\n",
    "        label=\"Current Memory State\",\n",
    "        lines=15,\n",
    "        interactive=False,\n",
    "        visible=False\n",
    "    )\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        bot_message = chat_function(user_message, history[:-1])\n",
    "        history[-1][1] = bot_message\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting Gradio interface with agent...\")\n",
    "    demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3aM-g3B1miRF",
    "lIZBtqS0cFQb"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
